#!/usr/bin/env python
# coding: utf-8
"""
unified_gemini_run.py
---------------------
ä¿®æ”¹ç‰ˆï¼šå»æ‰ combination é€»è¾‘ï¼Œç»Ÿä¸€ä½¿ç”¨ gemini-2.5-flash æ¨¡å‹
æ–°å¢ï¼šç­”æ¡ˆè´¨é‡æ£€æŸ¥ï¼ˆå­—æ•°å’Œæ ‡ç‚¹ç¬¦å·æ£€æŸ¥ï¼‰
"""

import csv, json, time, traceback
from pathlib import Path
from openai import OpenAI

# ===== 0. è·¯å¾„é…ç½® ===========================================================
BASE_DIR = Path(r"D:\project7\prompt")
BASE_DIR_1= Path(r"D:\project7\MM\result")
OUTPUT_DIR=Path(r"D:\project7\MM\result")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# è¾“å‡ºæ–‡ä»¶å®šä¹‰ï¼ˆç§»åˆ°æœ€å‰é¢ï¼‰
OUTPUT_FILE = OUTPUT_DIR / "answer-2+2-1-1-700.json"

PROMPT_CSV   = BASE_DIR_1 / "final_prompt_2+2-1-700.csv"
GROUPED_JSON = BASE_DIR / "multi_model_answers-1-700.json"
SAVE_INTERVAL = 10  # æ¯ N é¢˜ä¿å­˜ä¸€æ¬¡

# ===== 1. æ¨¡å‹é…ç½® =======================================================
# åªä¿ç•™ gemini æ¨¡å‹é…ç½®
MODEL_NAME = "gemini-2.5-flash"
API_KEY = "sk-eU0JtXoQSn3wSM0yA981lTMrUEDD31vtxAtFLA2ub6lwi3dd"
BASE_URL = "https://api.aigptapi.com/v1/"

# ç­”æ¡ˆè´¨é‡æ£€æŸ¥å‚æ•°
MIN_ANSWER_LENGTH = 10  # æœ€å°å­—æ•°è¦æ±‚
VALID_END_PUNCTUATION = {'.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ', ')', 'ï¼‰', '"', '"', "'", "'"}  # æœ‰æ•ˆçš„ç»“å°¾æ ‡ç‚¹

# ===== 2. IO & GPT è°ƒç”¨ ======================================================
def load_cache(path: Path):
    if not path.exists():
        return {}
    try:
        data = json.loads(path.read_text("utf-8"))
        cache = {}
        for q, entry in data.items():
            for m, ans in entry.get("basic_answers", []):
                cache.setdefault(q, {})[m] = ans
        return cache
    except Exception as e:
        print(f"âš ï¸ è¯»å– cache å¤±è´¥: {e}")
        return {}

def check_answer_quality(answer: str) -> tuple[bool, str]:
    """
    æ£€æŸ¥ç­”æ¡ˆè´¨é‡
    è¿”å›: (æ˜¯å¦åˆæ ¼, é—®é¢˜æè¿°)
    """
    if not answer:
        return False, "ç­”æ¡ˆä¸ºç©º"
    
    # æ£€æŸ¥å­—æ•°
    if len(answer.strip()) < MIN_ANSWER_LENGTH:
        return False, f"ç­”æ¡ˆè¿‡çŸ­ï¼ˆå°‘äº{MIN_ANSWER_LENGTH}å­—ï¼‰"
    
    # æ£€æŸ¥ç»“å°¾æ ‡ç‚¹
    last_char = answer.strip()[-1] if answer.strip() else ''
    if last_char not in VALID_END_PUNCTUATION:
        return False, f"ç­”æ¡ˆæœªä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼ˆæœ€åå­—ç¬¦: '{last_char}'ï¼‰"
    
    return True, "åˆæ ¼"

def ask(api: OpenAI, model: str, prompt: str, retry=3, pause=2):
    """
    è°ƒç”¨ API è·å–å›ç­”ï¼Œå¹¶è¿›è¡Œè´¨é‡æ£€æŸ¥
    """
    for i in range(retry):
        try:
            rsp = api.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                timeout=60
            )
            txt = rsp.choices[0].message.content.strip()
            
            # æ£€æŸ¥ç­”æ¡ˆè´¨é‡
            is_valid, issue = check_answer_quality(txt)
            if not is_valid:
                print(f"  âš ï¸ ç­”æ¡ˆè´¨é‡é—®é¢˜: {issue}")
                if i < retry - 1:
                    print(f"  ğŸ”„ é‡è¯•ä¸­...")
                    time.sleep(pause)
                    continue
                else:
                    print(f"  âŒ å¤šæ¬¡å°è¯•åä»æœ‰è´¨é‡é—®é¢˜ï¼Œä½¿ç”¨å½“å‰ç»“æœ")
            
            return txt
            
        except Exception as e:
            print(f"âŒ {model} ç¬¬ {i+1} æ¬¡å¤±è´¥: {e}")
            time.sleep(pause)
    return ""

def load_progress(file: Path):
    if not file.exists():
        return {}
    try:
        with file.open("r", encoding="utf-8") as f:
            data = json.load(f)
        return {row["question"]: row for row in data}
    except Exception as e:
        print(f"âš ï¸ è¯»å–è¿›åº¦å¤±è´¥: {e}")
        return {}

def save_progress(results: list, file: Path):
    """ä¿å­˜è¿›åº¦"""
    try:
        tmp = file.with_suffix(".tmp")
        tmp.write_text(json.dumps(results, ensure_ascii=False, indent=2), "utf-8")
        tmp.replace(file)
        print(f"ğŸ’¾ ä¿å­˜ {file.name} ï¼ˆ{len(results)} æ¡ï¼‰")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

# ===== 3. è„šæœ¬å…¥å£ï¼šç»Ÿä¸€ä½¿ç”¨ gemini å¤„ç†æ‰€æœ‰é—®é¢˜ ===============================
if __name__ == "__main__":
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    print(f"ğŸ“ ç­”æ¡ˆè´¨é‡è¦æ±‚: æœ€å°‘{MIN_ANSWER_LENGTH}å­—ï¼Œéœ€ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾")
    
    # 1) é¢„è¯» CSVï¼Œæ„å»º {question: [prompts]}
    q2prompts = {}
    with PROMPT_CSV.open("r", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            q = row["question"]
            prompt = row["prompt"]
            q2prompts.setdefault(q, []).append(prompt)

    all_questions = sorted(q2prompts.keys())
    print(f"ğŸ“š é¢˜ç›®æ•°: {len(all_questions)}")

    # 2) åˆå§‹åŒ– API å¯¹è±¡
    api = OpenAI(api_key=API_KEY, base_url=BASE_URL)
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    
    # 3) åŠ è½½å·²æœ‰è¿›åº¦
    existing_results = []
    processed_questions = set()
    
    if OUTPUT_FILE.exists():
        with OUTPUT_FILE.open("r", encoding="utf-8") as f:
            existing_results = json.load(f)
            processed_questions = {item["question"] for item in existing_results}
        print(f"ğŸ“Š å·²æœ‰è¿›åº¦: {len(processed_questions)} é¢˜")

    cache = load_cache(GROUPED_JSON)
    processed = 0
    skipped = 0
    quality_issues = 0

    # ------- ä¸»å¾ªç¯ï¼šé¢˜ç›®ä¼˜å…ˆ -----------------
    for qi, q in enumerate(all_questions, 1):
        print(f"\nğŸ“ [{qi}/{len(all_questions)}] {q[:60]}â€¦")
        
        # å·²å¤„ç†è¿‡åˆ™è·³è¿‡
        if q in processed_questions:
            print(f"  â­ï¸ å·²å¤„ç†è¿‡ï¼Œè·³è¿‡")
            skipped += 1
            continue

        item = {
            "question": q,
            "model": MODEL_NAME,
            "prompts": q2prompts[q],
            "answers": []
        }

        # å…ˆè·å–ç›´æ¥å›ç­”ï¼ˆä½¿ç”¨åŸå§‹é—®é¢˜ï¼‰
        print(f"  â””â”€ è·å–ç›´æ¥å›ç­”...")
        direct_answer = cache.get(q, {}).get(MODEL_NAME) or ask(api, MODEL_NAME, q)
        item["direct_answer"] = direct_answer
        
        # æ£€æŸ¥ç›´æ¥å›ç­”çš„è´¨é‡
        is_valid, issue = check_answer_quality(direct_answer)
        if not is_valid:
            quality_issues += 1
            item["direct_answer_quality_issue"] = issue
        
        # å¤„ç†è¯¥é¢˜çš„æ‰€æœ‰ prompts
        for i, prompt in enumerate(q2prompts[q], 1):
            print(f"  â””â”€ å¤„ç† prompt {i}/{len(q2prompts[q])}")
            
            reply = ask(api, MODEL_NAME, prompt)
            
            # æ£€æŸ¥å›ç­”è´¨é‡
            is_valid, issue = check_answer_quality(reply)
            answer_data = {
                "prompt_index": i,
                "prompt": prompt,
                "reply": reply
            }
            
            if not is_valid:
                quality_issues += 1
                answer_data["quality_issue"] = issue
            
            item["answers"].append(answer_data)
            
            # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            if i < len(q2prompts[q]):
                time.sleep(0.5)

        existing_results.append(item)
        processed += 1
        processed_questions.add(q)
        
        # ---- SAVE_INTERVAL ----
        if processed > 0 and processed % SAVE_INTERVAL == 0:
            print(f"\nğŸ’¾ è¾¾åˆ°ä¿å­˜é—´éš”ï¼Œä¿å­˜è¿›åº¦...")
            save_progress(existing_results, OUTPUT_FILE)

    # 4) å…¨éƒ¨å®Œæˆåä¿å­˜ä¸€æ¬¡
    print(f"\nğŸ å¤„ç†å®Œæˆï¼æ–°å¤„ç† {processed} é¢˜ï¼Œè·³è¿‡ {skipped} é¢˜")
    save_progress(existing_results, OUTPUT_FILE)
    print(f"âœ… æ€»è®¡ {len(existing_results)} æ¡è®°å½•")

    # ç»Ÿè®¡ä¿¡æ¯
    total_prompts = sum(len(item["answers"]) for item in existing_results)
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
    print(f"  - æ€»é¢˜ç›®æ•°: {len(existing_results)}")
    print(f"  - æ€» prompt æ•°: {total_prompts}")
    print(f"  - å¹³å‡æ¯é¢˜ prompts: {total_prompts/len(existing_results):.2f}")
    print(f"  - ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    print(f"  - è´¨é‡é—®é¢˜æ•°: {quality_issues}")

    # è´¨é‡é—®é¢˜æ±‡æ€»
    if quality_issues > 0:
        print(f"\nâš ï¸ å‘ç° {quality_issues} ä¸ªç­”æ¡ˆè´¨é‡é—®é¢˜")
        print("  å¯åœ¨è¾“å‡ºæ–‡ä»¶ä¸­æŸ¥çœ‹å…·ä½“é—®é¢˜è¯¦æƒ…ï¼ˆquality_issue å­—æ®µï¼‰")

    print(f"\nğŸ‰ å¤„ç†å®Œæ¯•ï¼Œæ–‡ä»¶ä¿å­˜åœ¨: {OUTPUT_FILE}")