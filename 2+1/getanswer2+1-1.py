#!/usr/bin/env python
# coding: utf-8
"""
multmm2_run2plus2_topic_first.py
--------------------------------
Process in the order of "question -> model":
  1. Read JSON data generated by the second file
  2. For the same question, call all models in sequence
  3. Write progress files of each model to disk every SAVE_INTERVAL questions
  4. Use the prompt field from the second file as input
  5. Retrieve direct replies from the original JSON file without repeated API calls

Modification: Adapt to the output format of the second file and extract answers from original data
"""

import csv, json, time, re, traceback
from pathlib import Path
from openai import OpenAI

# ========== Output Configuration (placed at the top for easy modification) ==========
OUTPUT_DIR = Path(r"D:\project7\MM\result\2+1")  # <-- Modify here to set output directory
OUTPUT_FILE_SUFFIX = "2+1-1-7800-8100"  # <-- Output file suffix, e.g.: gemini-2.5-flash_answers_2+1-1.json
# =====================================================

# ===== 0. Path Configuration ===========================================================
BASE_DIR   = Path(r"D:\project7\MM\result")

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Read JSON files of each combination
PROMPT_FILES = [
    BASE_DIR / "finalprompt_combination_1_2+1_7800-8100.json",
    # BASE_DIR / "finalprompt_combination_2_2+1.json",
    # BASE_DIR / "finalprompt_combination_3_2+1.json",
]

# JSON file with original questions and answers
ORIGINAL_JSON = Path(r"D:\project7\multi_model_answers7800-8100.json")
SAVE_INTERVAL = 1  # Save every N questions

# ===== 1. Model Account Configuration =======================================================
MODEL_CFGS = [
    {
        "model_name": "gemini-2.5-flash",
        "api_key": "sk-VJrRRrYljSfcLQPKD2ocOw8NrKaFOPsTszZy1gb5qWJixq2Y",
        "base_url": "https://api.aigptapi.com/v1/"
    },
    # {
    #     "model_name": "moonshot-v1-8k",
    #     "api_key": "sk-N4rH9BjW8xR1akf0C01426F958D74c9d97Bd7a131a09B5B4",
    #     "base_url": "https://api.vansai.cn/v1",
    # }
]

# ===== 2. Helper Functions ==================================================

def print_model_info(combo: str, cur: str):
    """Simplified combination information printing"""
    print(f"  â””â”€ Processing combination: {combo}")

# ===== 3. IO & GPT Calls ======================================================

def load_original_answers(path: Path):
    """Load all models' answers from the original JSON file"""
    if not path.exists():
        print(f"âš ï¸ Original answer file does not exist: {path}")
        return {}
    
    try:
        with path.open("r", encoding="utf-8") as f:
            data = json.load(f)
        
        answers_cache = {}
        if "questions" in data:
            for question, question_data in data["questions"].items():
                if "answers" in question_data:
                    answers_cache[question] = {}
                    for model, model_answers in question_data["answers"].items():
                        if model_answers and len(model_answers) > 0:
                            # Take the first answer
                            answer_text = model_answers[0].get("answer", "").strip()
                            if answer_text:
                                answers_cache[question][model] = answer_text
        
        print(f"âœ“ Successfully loaded original answers, containing {len(answers_cache)} questions")
        return answers_cache
        
    except Exception as e:
        print(f"âŒ Failed to read original answer file: {e}")
        return {}

def get_direct_answer(question: str, model_name: str, original_answers: dict):
    """Retrieve the specified model's direct answer to the question from original answers"""
    if question not in original_answers:
        print(f"  âš ï¸ Question not in original answers: {question[:50]}...")
        return ""
    
    question_answers = original_answers[question]
    
    # Try exact match
    if model_name in question_answers:
        return question_answers[model_name]
    
    # Try fuzzy match
    for model, answer in question_answers.items():
        if model_name.lower() in model.lower() or model.lower() in model_name.lower():
            print(f"  ðŸ“‹ Fuzzy match: {model_name} -> {model}")
            return answer
    
    print(f"  âš ï¸ No answer found for model {model_name}")
    return ""

# ---- New: Answer quality check function ----
END_PUNCT = ('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')
MIN_GOOD_LENGTH = 200  # Minimum length to trigger retry

def is_low_quality_answer(text: str) -> bool:
    if not text:
        return True
    text = text.strip()
    if len(text) < MIN_GOOD_LENGTH:
        return True
    if not text.endswith(END_PUNCT):
        return True
    return False

def ask(api: OpenAI, model: str, prompt: str, retry: int = 3, pause: int = 2):
    last_txt = ""
    for i in range(retry):
        try:
            rsp = api.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                timeout=60,
            )
            txt = (rsp.choices[0].message.content or "").strip()
            last_txt = txt

            if not is_low_quality_answer(txt):
                return txt

            print(f"âš ï¸ {model} returned too short/no ending punctuation on attempt {i+1} (len={len(txt)}), retrying...")
        except Exception as e:
            print(f"âŒ {model} failed on attempt {i+1}: {e}")

        time.sleep(pause)

    return last_txt

def load_progress(file: Path):
    if not file.exists():
        return {}
    try:
        with file.open("r", encoding="utf-8") as f:
            data = json.load(f)
        return {row["question"]: row for row in data}
    except Exception as e:
        print(f"âš ï¸ Failed to read progress: {e}")
        return {}

def save_progress(rows: list, file: Path):
    try:
        tmp = file.with_suffix(".tmp")
        tmp.write_text(json.dumps(rows, ensure_ascii=False, indent=2), "utf-8")
        tmp.replace(file)
        print(f"ðŸ’¾ Saved {file.name} ({len(rows)} entries)")
    except Exception as e:
        print(f"âŒ Save failed: {e}")

# ===== 4. Read prompt data generated by the second file ====================================

def load_prompts_from_files(prompt_files):
    """Read JSON files generated by the second file
    Returns {question: {combination: entry_data}}
    """
    q2data = {}
    
    for file_path in prompt_files:
        if not file_path.exists():
            print(f"âš ï¸ File does not exist: {file_path}")
            continue
            
        print(f"ðŸ“– Reading file: {file_path}")
        
        try:
            with file_path.open("r", encoding="utf-8") as f:
                entries = json.load(f)
            
            for entry in entries:
                q = entry["question"]
                combo = entry["combination"]
                
                if q not in q2data:
                    q2data[q] = {}
                
                # Store the entire entry data
                q2data[q][combo] = entry
                
            print(f"  â””â”€ Read {len(entries)} records")
            
        except Exception as e:
            print(f"âŒ Failed to read file {file_path}: {e}")
    
    return q2data

# ===== 5. Script Entry: Traverse by Question ==============================================
if __name__ == "__main__":
    print(f"ðŸ“ Output directory: {OUTPUT_DIR}")
    print(f"ðŸ“„ Output file suffix: {OUTPUT_FILE_SUFFIX}")
    print("-" * 60)
    
    # 1) Read data generated by the second file
    q2data = load_prompts_from_files(PROMPT_FILES)
    all_questions = sorted(q2data.keys())
    print(f"\nðŸ“š Number of questions: {len(all_questions)}")
    
    # 2) Read original answer data
    original_answers = load_original_answers(ORIGINAL_JSON)

    # 3) Prepare API, progress files, and row caches for each model
    model_env = {}
    for cfg in MODEL_CFGS:
        name = cfg["model_name"]
        output_file = OUTPUT_DIR / f"{name}_answers_{OUTPUT_FILE_SUFFIX}.json"
        model_env[name] = {
            "api": OpenAI(api_key=cfg["api_key"], base_url=cfg["base_url"]),
            "out": output_file,
            "rows": [],  # New/accumulated results
            "done": load_progress(output_file),
        }

    processed = 0

    # ------- Main Loop: Question Priority -----------------
    for qi, q in enumerate(all_questions, 1):
        print(f"\nðŸ“ [{qi}/{len(all_questions)}] {q[:60]}â€¦")
        
        # Get all combination data for this question
        q_combinations = q2data[q]

        for cfg in MODEL_CFGS:
            mname = cfg["model_name"]
            env = model_env[mname]

            # Skip if already processed
            if q in env["done"]:
                continue

            api = env["api"]

            print(f"ðŸ¤– Processing {mname}")
            # Retrieve the model's direct answer to the question from original answers
            direct_answer = get_direct_answer(q, mname, original_answers)
            item = {
                "question": q,
                "direct_prompt": q,
                "direct_reply": direct_answer,
            }

            # Process all combinations for this question
            for combo, entry_data in q_combinations.items():
                print_model_info(combo, mname)
                
                # Use the prompt field from the second file
                prompt_text = entry_data["prompt"]
                reply = ask(api, mname, prompt_text)
                
                # Save prompt and reply
                item[f"{combo}_prompt"] = prompt_text
                item[f"{combo}_reply"] = reply
            
            # After processing all combinations, add third_model and third_answer
            # Retrieve from any combination (since all combinations for the same question should have the same third information)
            if q_combinations:
                first_combo_data = next(iter(q_combinations.values()))
                if "third_model" in first_combo_data:
                    item["third_model"] = first_combo_data["third_model"]
                if "third_answer" in first_combo_data:
                    item["third_answer"] = first_combo_data["third_answer"]

            env["rows"].append(item)
            env["done"][q] = item  # Mark as completed

        processed += 1
        # ---- SAVE_INTERVAL ----
        if processed % SAVE_INTERVAL == 0:
            for mname, env in model_env.items():
                save_progress(env["rows"] + list(env["done"].values()), env["out"])

    # 4) Save once after all completion
    for mname, env in model_env.items():
        final_rows = env["rows"] + list(env["done"].values())
        save_progress(final_rows, env["out"])

    print("\nðŸŽ‰ All processing completed in question order, files saved in:", OUTPUT_DIR)
