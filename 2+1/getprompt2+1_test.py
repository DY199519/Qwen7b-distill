#!/usr/bin/env python
# coding: utf-8
"""
multmm1_build_prompts.py
------------------------
è¯»å–JSONæ–‡ä»¶å¹¶ç”Ÿæˆpromptï¼š
  Â· è¯»å–åŒ…å«questionså’Œanswersçš„JSONæ–‡ä»¶
  Â· æŒ‰ä¸åŒæ¨¡å‹ç»„åˆæå–ç­”æ¡ˆ
  Â· æ£€æŸ¥ç­”æ¡ˆè´¨é‡ï¼Œè¿‡æ»¤ä¸åˆæ ¼çš„ç­”æ¡ˆ
  Â· ä»å¤–éƒ¨æ–‡ä»¶è¯»å–promptæ¨¡æ¿
  Â· ä¸ºæ¯ä¸ªç»„åˆç”Ÿæˆç‹¬ç«‹çš„JSONæ–‡ä»¶
  Â· å°†ç»„åˆä¸­æœ€åä¸€ä¸ªæ¨¡å‹çš„ç­”æ¡ˆå•ç‹¬ä¿å­˜
"""

import json, csv
from pathlib import Path
import re
from typing import Tuple, List, Dict, Any
from datetime import datetime

# ========== è¾“å‡ºé…ç½®ï¼ˆæ”¾åœ¨æœ€å‰é¢ï¼Œæ–¹ä¾¿ä¿®æ”¹ï¼‰ ==========
OUTPUT_DIR = Path(r"D:\qwensft\2+1")  # <-- ä¿®æ”¹è¿™é‡Œè®¾ç½®è¾“å‡ºç›®å½•
OUTPUT_FILE_PREFIX = "finalprompt"  # <-- è¾“å‡ºæ–‡ä»¶å‰ç¼€
OUTPUT_FILE_SUFFIX = "2+1_test-1"  # <-- è¾“å‡ºæ–‡ä»¶åç¼€
# =====================================================

# === 1. è·¯å¾„é…ç½® ===
BASE_DIR = Path(r"D:\project7\prompt")
json_path = Path(r"D:\qwensft\testquestion\multi_model_answersTest500.json")

# Prompt æ–‡ä»¶è·¯å¾„
PROMPT_FILE = BASE_DIR / "prompt-2+1-1.txt"

# æ¨¡å‹ç»„åˆé…ç½®
MODEL_COMBINATIONS = {
    "combination_1": ["gemini", "grok", "doubao"],
    # "combination_2": ["moonshot", "Yi", "gpt"],
    # "combination_3": ["llama", "vucina"],
}

# === 2. è´¨é‡æ£€æŸ¥å‚æ•° ===
MIN_ANSWER_LENGTH = 100  # æœ€å°ç­”æ¡ˆé•¿åº¦
MIN_COMPLETE_LENGTH = 50  # å®Œæ•´æ€§æœ€å°é•¿åº¦

# === 3. ç­”æ¡ˆè´¨é‡æ£€æŸ¥å‡½æ•° ===
def check_answer_quality(answer_text: str) -> Tuple[bool, str]:
    """
    æ£€æŸ¥ç­”æ¡ˆè´¨é‡ï¼ˆä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„æ ‡å‡†ï¼‰
    è¿”å›: (æ˜¯å¦åˆæ ¼, é—®é¢˜æè¿°)
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
    if not answer_text or answer_text.strip() == "":
        return False, "ç©ºç­”æ¡ˆ"
    
    answer_text = answer_text.strip()
    
    # æ£€æŸ¥é•¿åº¦
    if len(answer_text) < MIN_COMPLETE_LENGTH:
        return False, f"ç­”æ¡ˆè¿‡çŸ­({len(answer_text)}å­—ç¬¦)"
    
    # ç®€å•æ£€æŸ¥ï¼šæ˜¯å¦ä»¥å¸¸è§çš„å®Œæ•´æ ‡ç‚¹ç»“å°¾
    if answer_text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
        return True, "å®Œæ•´"
    
    # å¦‚æœæ²¡æœ‰æ ‡ç‚¹ç»“å°¾ï¼Œæ£€æŸ¥é•¿åº¦
    if len(answer_text) < MIN_ANSWER_LENGTH:
        return False, f"æ— ç»“å°¾æ ‡ç‚¹ä¸”è¾ƒçŸ­({len(answer_text)}å­—ç¬¦)"
    
    # é•¿ç­”æ¡ˆä½†æ— æ ‡ç‚¹ï¼Œä¹Ÿè§†ä¸ºä¸å®Œæ•´
    return False, f"æ— ç»“å°¾æ ‡ç‚¹({len(answer_text)}å­—ç¬¦)"

# === 4. è¯»å– Prompt æ¨¡æ¿ ===
def load_prompt_template():
    """ä»æ–‡ä»¶è¯»å– prompt æ¨¡æ¿"""
    try:
        with open(PROMPT_FILE, 'r', encoding='utf-8') as f:
            template = f.read().strip()
            print(f"âœ“ æˆåŠŸè¯»å– prompt æ¨¡æ¿ï¼š{PROMPT_FILE}")
            return template
    except FileNotFoundError:
        print(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ° prompt æ–‡ä»¶ï¼š{PROMPT_FILE}")
        # ä½¿ç”¨é»˜è®¤æ¨¡æ¿ä½œä¸ºå¤‡ä»½
        default_template = 'è¯·å›ç­”ï¼š"{q}"ï¼ŒåŸºäºä»¥ä¸‹å›ç­”å¯¹ä½ çš„ç­”æ¡ˆè¿›è¡Œå®Œå–„ï¼š{ctx}ã€‚'
        print(f"  ä½¿ç”¨é»˜è®¤ prompt æ¨¡æ¿")
        return default_template

# === 5. å·¥å…·å‡½æ•° ===
def fuzzy_match_model(model_pattern, available_models):
    """æ¨¡ç³ŠåŒ¹é…æ¨¡å‹åç§°ï¼Œè¿”å›åŒ¹é…çš„æ¨¡å‹åˆ—è¡¨"""
    matched_models = []
    for model in available_models:
        if model_pattern.lower() in model.lower():
            matched_models.append(model)
    return matched_models

def extract_answers_with_quality_check(question_data, model_patterns):
    """
    ä»é—®é¢˜æ•°æ®ä¸­æå–æŒ‡å®šæ¨¡å‹æ¨¡å¼çš„ç­”æ¡ˆï¼Œå¹¶è¿›è¡Œè´¨é‡æ£€æŸ¥
    è¿”å›: (ç­”æ¡ˆåˆ—è¡¨, æ‰¾åˆ°çš„æ¨¡å‹åˆ—è¡¨, è´¨é‡é—®é¢˜åˆ—è¡¨)
    """
    answers = []
    found_models = []
    quality_issues = []
    
    if "answers" in question_data:
        available_models = list(question_data["answers"].keys())
        
        for pattern in model_patterns:
            # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°ç¬¦åˆæ¨¡å¼çš„æ¨¡å‹
            matched_models = fuzzy_match_model(pattern, available_models)
            
            # ä»åŒ¹é…çš„æ¨¡å‹ä¸­é€‰æ‹©ç¬¬ä¸€ä¸ªæœ‰æ•ˆç­”æ¡ˆ
            found_valid = False
            for model in matched_models:
                if model in question_data["answers"]:
                    model_answers = question_data["answers"][model]
                    if model_answers and len(model_answers) > 0:
                        # åªå–ç¬¬ä¸€ä¸ªç­”æ¡ˆ
                        answer_text = model_answers[0].get("answer", "").strip()
                        
                        # è´¨é‡æ£€æŸ¥
                        is_quality_good, issue_desc = check_answer_quality(answer_text)
                        
                        if is_quality_good:
                            answers.append(answer_text)
                            found_models.append(model)
                            found_valid = True
                            break
                        else:
                            quality_issues.append({
                                "model": model,
                                "issue": issue_desc,
                                "answer_preview": answer_text[:50] + "..." if len(answer_text) > 50 else answer_text
                            })
            
            # å¦‚æœè¿™ä¸ªæ¨¡å¼æ²¡æœ‰æ‰¾åˆ°åˆæ ¼çš„ç­”æ¡ˆï¼Œè®°å½•é—®é¢˜
            if not found_valid:
                quality_issues.append({
                    "model_pattern": pattern,
                    "issue": "æœªæ‰¾åˆ°è´¨é‡åˆæ ¼çš„ç­”æ¡ˆ"
                })
    
    return answers, found_models, quality_issues

def build_records(questions_data, prompt_template, combo_name, model_patterns):
    """ä¸ºå•ä¸ªç»„åˆæ„é€ è®°å½•åˆ—è¡¨ï¼ŒåŒ…å«è´¨é‡æ£€æŸ¥"""
    rows = []
    combo_count = 0
    skipped_count = 0
    quality_issues_summary = {}
    
    print(f"\n  å¼€å§‹è´¨é‡æ£€æŸ¥...")
    
    for question, question_data in questions_data.items():
        # æå–å½“å‰ç»„åˆæ¨¡å‹çš„ç­”æ¡ˆå¹¶è¿›è¡Œè´¨é‡æ£€æŸ¥
        answers, found_models, quality_issues = extract_answers_with_quality_check(
            question_data, model_patterns
        )
        
        # è®°å½•è´¨é‡é—®é¢˜
        if quality_issues:
            quality_issues_summary[question] = quality_issues
        
        # æ–°çš„promptæ ¼å¼éœ€è¦è‡³å°‘2ä¸ªè´¨é‡åˆæ ¼çš„ç­”æ¡ˆ
        if len(answers) < 2:
            skipped_count += 1
            continue
        
        # ç”Ÿæˆ prompt
        try:
            if len(answers) >= 2:
                prompt = prompt_template.format(q=question, A1=answers[0], A2=answers[1])
            else:
                continue
        except KeyError as e:
            # å¦‚æœæ¨¡æ¿æ ¼å¼ä¸åŒ¹é…ï¼Œå°è¯•æ—§æ ¼å¼
            ctx = "\n".join(f"å›ç­”{i+1}ï¼š{ans}" for i, ans in enumerate(answers[:2]))
            try:
                prompt = prompt_template.format(q=question, ctx=ctx)
            except:
                print(f"  âš ï¸ è­¦å‘Šï¼špromptæ¨¡æ¿æ ¼å¼ä¸åŒ¹é…ï¼Œè·³è¿‡é—®é¢˜ï¼š{question[:50]}...")
                skipped_count += 1
                continue
        
        # æ„å»ºè®°å½•
        record = {
            "question": question,
            "prompt": prompt,
            "model": ",".join(found_models[:2]),
            "version": f"{combo_name}_{min(len(answers), 2)}_answers",
            "combination": combo_name,
            "answer_quality": "checked"  # æ ‡è®°å·²é€šè¿‡è´¨é‡æ£€æŸ¥
        }
        
        # å¦‚æœæœ‰ç¬¬ä¸‰ä¸ªç­”æ¡ˆ
        if len(answers) >= 3 and len(found_models) >= 3:
            record["third_model"] = found_models[2]
            record["third_answer"] = answers[2]
        
        rows.append(record)
        combo_count += 1
    
    print(f"  Â· {combo_name} ç”Ÿæˆ {combo_count} æ¡è®°å½•")
    print(f"  Â· å› è´¨é‡é—®é¢˜è·³è¿‡ {skipped_count} æ¡è®°å½•")
    
    # å¦‚æœæœ‰è´¨é‡é—®é¢˜ï¼Œè¾“å‡ºè¯¦ç»†æŠ¥å‘Š
    if quality_issues_summary:
        issue_count = len(quality_issues_summary)
        print(f"  Â· å‘ç° {issue_count} ä¸ªé—®é¢˜å­˜åœ¨è´¨é‡é—®é¢˜")
        
        # ä¿å­˜è´¨é‡é—®é¢˜æŠ¥å‘Š
        quality_report_file = OUTPUT_DIR / f"quality_report_{combo_name}.json"
        with quality_report_file.open("w", encoding="utf-8") as f:
            json.dump({
                "combination": combo_name,
                "total_questions": len(questions_data),
                "questions_with_issues": issue_count,
                "skipped_questions": skipped_count,
                "generated_prompts": combo_count,
                "quality_issues": quality_issues_summary
            }, f, ensure_ascii=False, indent=2)
        print(f"  Â· è´¨é‡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {quality_report_file}")
    
    return rows

# === 6. ä¸»ç¨‹åº ===
print("ğŸ“– å¼€å§‹å¤„ç†æ•°æ®...")
print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# è¯»å– prompt æ¨¡æ¿
prompt_template = load_prompt_template()

# è¯»å– JSON æ•°æ®
print(f"\nğŸ“– è¯»å– JSON æ–‡ä»¶ï¼š{json_path}")
try:
    with open(json_path, encoding="utf-8") as f:
        data = json.load(f)
    
    # æ£€æŸ¥æ•°æ®ç»“æ„
    if "questions" in data:
        questions_data = data["questions"]
        print(f"  Â· æ‰¾åˆ° {len(questions_data)} ä¸ªé—®é¢˜")
    else:
        print("âŒ é”™è¯¯ï¼šJSONæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ° 'questions' å­—æ®µ")
        exit(1)
        
except FileNotFoundError:
    print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {json_path}")
    exit(1)
except json.JSONDecodeError as e:
    print(f"âŒ é”™è¯¯ï¼šJSONè§£æå¤±è´¥ï¼š{e}")
    exit(1)

# === 7. ç”Ÿæˆè®°å½•å¹¶å†™å…¥ JSON ===
print("\nâš™ï¸ ç”Ÿæˆ prompt è®°å½•...")

total_count = 0
total_skipped = 0

# ä¸ºæ¯ä¸ªç»„åˆç”Ÿæˆç‹¬ç«‹çš„JSONæ–‡ä»¶
for combo_name, model_patterns in MODEL_COMBINATIONS.items():
    print(f"\nğŸ“‹ å¤„ç†ç»„åˆ {combo_name}: {', '.join(model_patterns)}")
    
    # ç”Ÿæˆå½“å‰ç»„åˆçš„è®°å½•
    combo_rows = build_records(questions_data, prompt_template, combo_name, model_patterns)
    
    if combo_rows:
        # ä¸ºæ¯ä¸ªç»„åˆåˆ›å»ºç‹¬ç«‹çš„JSONæ–‡ä»¶
        out_json = OUTPUT_DIR / f"{OUTPUT_FILE_PREFIX}_{combo_name}_{OUTPUT_FILE_SUFFIX}.json"
        
        # å†™å…¥JSONæ–‡ä»¶
        print(f"ğŸ“ å†™å…¥ JSON æ–‡ä»¶ï¼š{out_json}")
        with out_json.open("w", encoding="utf-8") as f:
            json.dump(combo_rows, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æˆåŠŸå†™å…¥ {len(combo_rows)} æ¡è®°å½•åˆ° {out_json.name}")
        total_count += len(combo_rows)
    else:
        print(f"âš ï¸ è­¦å‘Šï¼š{combo_name} æ²¡æœ‰ç”Ÿæˆä»»ä½•è®°å½•ï¼ˆæ‰€æœ‰ç­”æ¡ˆéƒ½æœªé€šè¿‡è´¨é‡æ£€æŸ¥ï¼‰")

# ç”Ÿæˆæ€»ä½“ç»Ÿè®¡æŠ¥å‘Š
summary_file = OUTPUT_DIR / f"{OUTPUT_FILE_PREFIX}_summary_{OUTPUT_FILE_SUFFIX}.json"
with summary_file.open("w", encoding="utf-8") as f:
    json.dump({
        "total_questions": len(questions_data),
        "total_prompts_generated": total_count,
        "combinations_processed": len(MODEL_COMBINATIONS),
        "quality_check_enabled": True,
        "min_answer_length": MIN_ANSWER_LENGTH,
        "min_complete_length": MIN_COMPLETE_LENGTH,
        "timestamp": datetime.now().isoformat()
    }, f, ensure_ascii=False, indent=2)

print(f"\nğŸ“Š æ€»è®¡ç”Ÿæˆ {total_count} æ¡è®°å½•ï¼Œåˆ†å¸ƒåœ¨ {len(MODEL_COMBINATIONS)} ä¸ªæ–‡ä»¶ä¸­")
print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡å·²ä¿å­˜åˆ°: {summary_file}")
print("\nğŸ‰ å®Œæˆï¼")