#!/usr/bin/env python
# coding: utf-8
"""
direct_rlhf_generator.py
ç›´æ¥ä»åŸå§‹fusionæ•°æ®å’Œè¯„åˆ†æ•°æ®ç”ŸæˆRLHFæ ¼å¼æ•°æ®é›†
"""
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any
import sys
import pandas as pd
# ==================== é…ç½®éƒ¨åˆ† - ä¿®æ”¹è¿™é‡Œ ====================
BASE_DIR = Path(r"D:\project7\10000final")
FUSION_JSON = BASE_DIR / "doubao-pro-32k_answers_2+1-2-1-9400.json"
SCORES_JSON = BASE_DIR / "grades_doubao-pro-256k_answers_2+1-2-1-9400.json"
OUTPUT_BASE = BASE_DIR / "rlhf_train_top100" # åŸºç¡€æ–‡ä»¶åï¼Œä¼šç”Ÿæˆ .json å’Œ .parquet
TOP_N = 100 # å–è¯„åˆ†æœ€é«˜çš„å‰Nä¸ª
# ===========================================================
# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)
def load_scores_data(scores_path: Path) -> Dict[str, Dict]:
    """åŠ è½½è¯„åˆ†æ•°æ®"""
    try:
        with scores_path.open("r", encoding="utf-8") as f:
            scores_data = json.load(f)
       
        question_scores = {}
        detailed_results = scores_data.get("detailed_results", [])
       
        for item in detailed_results:
            question = item.get("question", "")
            if question:
                avg_score_50 = item.get("avg_scores", {}).get("total", 0)
                question_scores[question] = {
                    "avg_score_50": avg_score_50,
                    "avg_scores": item.get("avg_scores", {}),
                    "num_valid_trials": item.get("num_valid_trials", 0),
                }
       
        logger.info(f"æˆåŠŸåŠ è½½äº† {len(question_scores)} ä¸ªé—®é¢˜çš„è¯„åˆ†æ•°æ®")
        return question_scores
       
    except Exception as e:
        logger.error(f"åŠ è½½è¯„åˆ†æ•°æ®å¤±è´¥: {e}")
        return {}
def create_rlhf_sample(item: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°†fusionæ•°æ®é¡¹è½¬æ¢ä¸ºRLHFæ ¼å¼
    """
    # problemå°±æ˜¯åŸå§‹çš„question
    problem_text = item.get("question", "")
   
    # è·å–fusion_promptå’Œfusion_reply
    fusion_prompt = item.get("fusion_prompt", "")
    fusion_reply = item.get("fusion_reply", "")
   
    # æ‹¼æ¥solutionï¼Œæ˜ç¡®æ ‡è®°ä¸¤éƒ¨åˆ†
    if fusion_prompt and fusion_reply:
        solution_text = (
            "ã€Fusion Promptã€‘\n"
            f"{fusion_prompt}\n\n"
            "ã€Fusion Replyã€‘\n"
            f"{fusion_reply}"
        )
    elif fusion_reply:
        solution_text = f"ã€Fusion Replyã€‘\n{fusion_reply}"
    else:
        solution_text = ""
   
    # æ„å»ºmessages - åŒ…å«å®Œæ•´å¯¹è¯
    messages = [
        {
            "role": "user",
            "content": problem_text
        },
        {
            "role": "assistant",
            "content": solution_text
        }
    ]
   
    # æ„å»ºRLHFæ ¼å¼çš„æ ·æœ¬
    rlhf_sample = {
        "problem": {"Value": problem_text},
        "solution": {"Value": solution_text},
        "messages": {"Value": messages} # ç›´æ¥å­˜å‚¨åˆ—è¡¨ï¼Œä¸æ˜¯JSONå­—ç¬¦ä¸²
    }
   
    return rlhf_sample
def generate_dataset():
    """ç”ŸæˆRLHFæ ¼å¼çš„æ•°æ®é›†"""
   
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not FUSION_JSON.exists():
        logger.error(f"Fusionæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {FUSION_JSON}")
        return 1
   
    if not SCORES_JSON.exists():
        logger.error(f"è¯„åˆ†æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {SCORES_JSON}")
        return 1
   
    # åŠ è½½è¯„åˆ†æ•°æ®
    logger.info("æ­£åœ¨åŠ è½½è¯„åˆ†æ•°æ®...")
    question_scores = load_scores_data(SCORES_JSON)
    if not question_scores:
        logger.error("æœªèƒ½åŠ è½½è¯„åˆ†æ•°æ®")
        return 1
   
    # åŠ è½½fusionæ•°æ®
    logger.info("æ­£åœ¨åŠ è½½fusionæ•°æ®...")
    with FUSION_JSON.open("r", encoding="utf-8") as f:
        data = json.load(f)
   
    if isinstance(data, dict):
        items = [data]
    elif isinstance(data, list):
        items = data
    else:
        logger.error(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {type(data)}")
        return 1
   
    logger.info(f"åŠ è½½äº† {len(items)} æ¡fusionæ•°æ®")
   
    # åŒ¹é…è¯„åˆ†å¹¶è¿‡æ»¤æ•°æ®
    items_with_scores = []
    required_fields = ["question", "fusion_reply"]
   
    for item in items:
        question = item.get("question", "")
       
        # æ£€æŸ¥æ˜¯å¦æœ‰è¯„åˆ†
        if question not in question_scores:
            continue
           
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        if not all(item.get(field) for field in required_fields):
            continue
       
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        question_len = len(item.get("question", ""))
        fusion_reply_len = len(item.get("fusion_reply", ""))
       
        if question_len < 10 or fusion_reply_len < 100:
            continue
       
        # æ·»åŠ è¯„åˆ†ä¿¡æ¯
        item["avg_score_50"] = question_scores[question]["avg_score_50"]
        items_with_scores.append(item)
   
    logger.info(f"æ‰¾åˆ° {len(items_with_scores)} æ¡æœ‰æ•ˆæ•°æ®ï¼ˆæœ‰è¯„åˆ†ä¸”å­—æ®µå®Œæ•´ï¼‰")
   
    # æŒ‰è¯„åˆ†æ’åºï¼Œå–TOP N
    items_with_scores.sort(key=lambda x: x["avg_score_50"], reverse=True)
    items_to_process = items_with_scores[:TOP_N]
   
    logger.info(f"\nå°†å¤„ç†å¾—åˆ†æœ€é«˜çš„ {len(items_to_process)} æ¡æ•°æ®")
    logger.info("\nTOP 5 é«˜åˆ†é—®é¢˜ï¼ˆ50åˆ†åˆ¶ï¼‰ï¼š")
    for i, item in enumerate(items_to_process[:5], 1):
        score = item["avg_score_50"]
        question = item["question"][:80]
        logger.info(f" {i}. å¾—åˆ†: {score:.2f}/50 - {question}...")
   
    # è½¬æ¢ä¸ºRLHFæ ¼å¼
    rlhf_dataset = []
    for idx, item in enumerate(items_to_process, 1):
        try:
            rlhf_sample = create_rlhf_sample(item)
           
            if rlhf_sample["problem"]["Value"] and rlhf_sample["solution"]["Value"]:
                rlhf_dataset.append(rlhf_sample)
               
                if idx == 1:
                    logger.info("\nâœ… ç¬¬ä¸€ä¸ªRLHFæ ·æœ¬ç¤ºä¾‹:")
                    logger.info("-" * 60)
                    logger.info(f"Problem (åŸå§‹é—®é¢˜): {rlhf_sample['problem']['Value'][:200]}...")
                    logger.info("-" * 60)
               
        except Exception as e:
            logger.error(f"å¤„ç†ç¬¬ {idx} ä¸ªæ ·æœ¬å¤±è´¥: {e}")
            continue
   
    if not rlhf_dataset:
        logger.error("æœªèƒ½ç”Ÿæˆä»»ä½•æœ‰æ•ˆçš„RLHFæ ·æœ¬")
        return 1
   
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    OUTPUT_BASE.parent.mkdir(parents=True, exist_ok=True)
   
    # ä¿å­˜ä¸ºJSONæ ¼å¼
    json_file = OUTPUT_BASE.with_suffix('.json')
    with json_file.open("w", encoding="utf-8") as f:
        json.dump(rlhf_dataset, f, ensure_ascii=False, indent=2)
    logger.info(f"âœ… JSONæ ¼å¼å·²ä¿å­˜åˆ°: {json_file}")
   
    # ä¿å­˜ä¸ºParquetæ ¼å¼
    parquet_file = OUTPUT_BASE.with_suffix('.parquet')
    # å°†åµŒå¥—çš„å­—å…¸ç»“æ„å±•å¹³ä¸ºDataFrame
    df_data = []
    for item in rlhf_dataset:
        df_data.append({
            "problem": item["problem"]["Value"],
            "solution": item["solution"]["Value"],
            "messages": item["messages"]["Value"] # ç›´æ¥å­˜å‚¨åˆ—è¡¨ï¼ŒArrowä¼šè‡ªåŠ¨å¤„ç†
        })
    df = pd.DataFrame(df_data)
    df.to_parquet(parquet_file, index=False)
    logger.info(f"âœ… Parquetæ ¼å¼å·²ä¿å­˜åˆ°: {parquet_file}")
   
    logger.info(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(rlhf_dataset)} ä¸ªRLHFè®­ç»ƒæ ·æœ¬")
   
    # ç»Ÿè®¡ä¿¡æ¯
    logger.info("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    problem_lengths = [len(item["problem"]["Value"]) for item in rlhf_dataset]
    solution_lengths = [len(item["solution"]["Value"]) for item in rlhf_dataset]
    logger.info(f" - æ ·æœ¬æ•°é‡: {len(rlhf_dataset)}")
    logger.info(f" - Problemå¹³å‡é•¿åº¦: {sum(problem_lengths)/len(problem_lengths):.0f} å­—ç¬¦")
    logger.info(f" - Solutionå¹³å‡é•¿åº¦: {sum(solution_lengths)/len(solution_lengths):.0f} å­—ç¬¦")
   
    # è¾“å‡º Excel: æ¯é“é¢˜å’Œè¯„åˆ†ï¼ŒæŒ‰ç…§è¯„åˆ†ä»é«˜å¾€ä¸‹
    scores_data = []
    for item in items_with_scores:
        scores_data.append({
            "question": item["question"],
            "avg_score_50": item["avg_score_50"]
        })
    df_scores = pd.DataFrame(scores_data)
    excel_file = OUTPUT_BASE.with_suffix('.xlsx')
    df_scores.to_excel(excel_file, index=False)
    logger.info(f"âœ… Excelæ ¼å¼å·²ä¿å­˜åˆ°: {excel_file}")
   
    return 0
def main():
    parser = argparse.ArgumentParser(description="ç”ŸæˆRLHFæ•°æ®é›†")
    parser.add_argument("--top", type=int, help="è¦†ç›–é»˜è®¤çš„TOP_Nå€¼")
    parser.add_argument("--output", type=str, help="è¦†ç›–é»˜è®¤çš„è¾“å‡ºåŸºç¡€æ–‡ä»¶å")
   
    args = parser.parse_args()
   
    # å¦‚æœæä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼Œè¦†ç›–é»˜è®¤å€¼
    if args.top:
        global TOP_N
        TOP_N = args.top
   
    if args.output:
        global OUTPUT_BASE
        OUTPUT_BASE = Path(args.output)
   
    logger.info("=" * 70)
    logger.info("RLHFæ•°æ®é›†ç”Ÿæˆå™¨")
    logger.info("=" * 70)
    logger.info(f"Fusionæ•°æ®: {FUSION_JSON}")
    logger.info(f"è¯„åˆ†æ•°æ®: {SCORES_JSON}")
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_BASE}.json å’Œ {OUTPUT_BASE}.parquet")
    logger.info(f"å¤„ç†æ•°é‡: TOP {TOP_N}")
    logger.info("=" * 70)
   
    return generate_dataset()
if __name__ == "__main__":
    sys.exit(main())