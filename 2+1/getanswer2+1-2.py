#!/usr/bin/env python
# coding: utf-8
"""
multmm2_run2plus2_topic_first_modified.py
----------------------------------------
ä¿®æ”¹ç‰ˆï¼šä» JSON æ ¼å¼ä¸­æå– third_answer ä½œä¸º A1ï¼Œcombination_1_reply ä½œä¸º A2
ä½¿ç”¨æ–°çš„æç¤ºè¯æ¨¡æ¿
æ·»åŠ ç­”æ¡ˆè´¨é‡æ£€æŸ¥åŠŸèƒ½
"""

import csv, json, time, re, traceback
from pathlib import Path
from openai import OpenAI
from datetime import datetime

# ===== 0. è¾“å‡ºè·¯å¾„å’Œæ–‡ä»¶åé…ç½® ======================================================
BASE_DIR   = Path(r"D:\project7\MM\result\2+1")
BASE_DIR_1   = Path(r"D:\project7\prompt")
OUTPUT_DIR = Path(r"D:\project7\MM\result\2+1")  # å¯ä»¥åœ¨è¿™é‡Œè½»æ¾ä¿®æ”¹è¾“å‡ºè·¯å¾„
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ========== è¾“å‡ºæ–‡ä»¶åé…ç½® - åœ¨è¿™é‡Œä¿®æ”¹ï¼ ==========
# æ–¹å¼1: ä½¿ç”¨å›ºå®šçš„åç¼€
OUTPUT_SUFFIX = "answers_2+1-2-7800-8100"  # ä¿®æ”¹è¿™ä¸ªå€¼æ¥æ”¹å˜è¾“å‡ºæ–‡ä»¶å

# æ–¹å¼2: ä½¿ç”¨æ—¶é—´æˆ³
# timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
# OUTPUT_SUFFIX = f"answers_{timestamp}"

# æ–¹å¼3: ä½¿ç”¨å®Œå…¨è‡ªå®šä¹‰çš„åç§°
# OUTPUT_FILENAME_TEMPLATE = "{model_name}_èåˆç»“æœ_v2.json"  # {model_name} ä¼šè¢«æ›¿æ¢ä¸ºæ¨¡å‹å

# æ–¹å¼4: ä¸ºæ¯ä¸ªæ¨¡å‹å•ç‹¬æŒ‡å®šè¾“å‡ºæ–‡ä»¶åï¼ˆåœ¨ MODEL_CFGS ä¸­æ·»åŠ  output_filename å­—æ®µï¼‰
# ================================================

# è¯»å–åŒ…å« combination_1_reply å’Œ third_answer çš„ JSON æ–‡ä»¶
ANSWER_FILE = BASE_DIR / "gemini-2.5-flash_answers_2+1-1-7800-8100.json"  # åŒ…å«é—®é¢˜ã€combination_1_reply å’Œ third_answer çš„æ–‡ä»¶
PROMPT_FILE = BASE_DIR_1 / "prompt-2+1-2.txt"  # æç¤ºè¯æ¨¡æ¿æ–‡ä»¶
SAVE_INTERVAL = 1  # æ¯ N é¢˜ä¿å­˜ä¸€æ¬¡

# ===== 1. æ¨¡å‹è´¦æˆ·é…ç½® =======================================================
MODEL_CFGS = [
    {
        "model_name": "doubao-pro-32k",
        "api_key": "sk-TlCq2TfX7oLuXzZMD1A3681285A2460bA26b6f0cEa5517Aa",
        "base_url": "https://vir.vimsai.com/v1",
        # "output_filename": "doubao_è‡ªå®šä¹‰è¾“å‡º.json"  # å¯é€‰ï¼šä¸ºç‰¹å®šæ¨¡å‹æŒ‡å®šè¾“å‡ºæ–‡ä»¶å
    }
]

# ===== 2. ç­”æ¡ˆè´¨é‡æ£€æŸ¥é…ç½® ===================================================
MIN_ANSWER_LENGTH = 10  # æœ€å°ç­”æ¡ˆé•¿åº¦
VALID_ENDINGS = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', ')', 'ï¼‰', '"', '"', "'", "'"]  # æœ‰æ•ˆçš„ç»“å°¾æ ‡ç‚¹
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°

def check_answer_quality(answer: str, question: str = ""):
    """
    æ£€æŸ¥ç­”æ¡ˆè´¨é‡
    è¿”å›: (is_valid, error_message)
    """
    if not answer or not answer.strip():
        return False, "ç­”æ¡ˆä¸ºç©º"
    
    answer = answer.strip()
    
    # æ£€æŸ¥é•¿åº¦
    if len(answer) < MIN_ANSWER_LENGTH:
        return False, f"ç­”æ¡ˆè¿‡çŸ­ï¼ˆ{len(answer)}å­—ç¬¦ï¼Œæœ€å°‘éœ€è¦{MIN_ANSWER_LENGTH}å­—ç¬¦ï¼‰"
    
    # æ£€æŸ¥æ˜¯å¦ä»¥åˆé€‚çš„æ ‡ç‚¹ç¬¦å·ç»“å°¾
    if not any(answer.endswith(ending) for ending in VALID_ENDINGS):
        return False, f"ç­”æ¡ˆå¯èƒ½è¢«æˆªæ–­ï¼Œç»“å°¾å­—ç¬¦: '{answer[-1] if answer else 'N/A'}'"
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„æˆªæ–­æ ‡å¿—
    truncation_signs = ['...', 'â€¦â€¦', '[æœªå®Œæˆ]', '[æˆªæ–­]', '(æœªå®Œ', 'ï¼ˆæœªå®Œ']
    if any(sign in answer for sign in truncation_signs):
        return False, "ç­”æ¡ˆåŒ…å«æˆªæ–­æ ‡å¿—"
    
    # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦è¿‡äºé‡å¤ï¼ˆå¯èƒ½æ˜¯ç”Ÿæˆå¼‚å¸¸ï¼‰
    words = answer.split()
    if len(words) > 5:
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        max_freq = max(word_freq.values())
        if max_freq > len(words) * 0.5:  # å¦‚æœæŸä¸ªè¯å‡ºç°è¶…è¿‡50%
            return False, "ç­”æ¡ˆå†…å®¹è¿‡äºé‡å¤"
    
    return True, "è´¨é‡æ£€æŸ¥é€šè¿‡"

# ===== 3. è¯»å–æç¤ºè¯æ¨¡æ¿ ====================================================
def load_prompt_template(template_path: Path):
    """è¯»å–æç¤ºè¯æ¨¡æ¿æ–‡ä»¶"""
    try:
        with template_path.open("r", encoding="utf-8") as f:
            return f.read().strip()
    except Exception as e:
        print(f"âŒ è¯»å–æç¤ºè¯æ¨¡æ¿å¤±è´¥: {e}")
        # å¦‚æœè¯»å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿
        return """é˜…è¯» å…¶ä»–ä¸¤ä¸ªæ¨¡å‹çš„æ€»ç»“å›ç­”A2 å¹¶å®Œå–„æ‚¨ä¹‹å‰çš„å›ç­”A1ã€‚å¦‚ä»æœ‰ç©ºç¼ºï¼Œå¯è¡¥å……ä½ çš„å¸¸è¯†æˆ–å…¬å¼€èµ„æ–™ï¼Œå¹¶ç”¨æ‹¬å·æ³¨æ˜æ¥æºï¼ˆå¸¸è¯†ï¼å…¬å¼€èµ„æ–™ï¼‰ã€‚
é—®é¢˜ï¼š
{q}
A1ï¼š
{A1}
A2ï¼š
{A2}
ã€ä»»åŠ¡è¯´æ˜ã€‘  
ä¸å±•ç¤ºä¸­é—´æå–è¿‡ç¨‹ã€‚å‰é¢ä¸å¸¦ä»»ä½•é“ºå«æ€§çš„è¯­å¥
ã€è¾“å‡ºè¦æ±‚ã€‘  
- æ¡ç†æ¸…æ™°ï¼Œå¯ä½¿ç”¨ç¼–å·æˆ–åˆ†æ®µï¼›  
- é¿å…èµ˜è¿°ï¼Œä¿æŒç®€ç»ƒã€‚"""

# åŠ è½½æç¤ºè¯æ¨¡æ¿
PROMPT_TEMPLATE = load_prompt_template(PROMPT_FILE)

# ===== 4. è¾…åŠ©å‡½æ•° ==========================================================

def get_output_filename(model_name: str, cfg: dict):
    """æ ¹æ®é…ç½®ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å"""
    # ä¼˜å…ˆä½¿ç”¨æ¨¡å‹é…ç½®ä¸­çš„è‡ªå®šä¹‰æ–‡ä»¶å
    if "output_filename" in cfg:
        return cfg["output_filename"]
    
    # ä½¿ç”¨å…¨å±€æ¨¡æ¿ï¼ˆå¦‚æœå®šä¹‰äº†ï¼‰
    if 'OUTPUT_FILENAME_TEMPLATE' in globals():
        return OUTPUT_FILENAME_TEMPLATE.format(model_name=model_name)
    
    # é»˜è®¤ä½¿ç”¨åç¼€æ–¹å¼
    return f"{model_name}_{OUTPUT_SUFFIX}.json"

def ask(api: OpenAI, model: str, prompt: str, retry: int = 3, pause: int = 2, question: str = ""):
    """è°ƒç”¨APIå¹¶è¿›è¡Œè´¨é‡æ£€æŸ¥"""
    for i in range(retry):
        try:
            rsp = api.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                timeout=60,
            )
            txt = rsp.choices[0].message.content.strip()
            
            # è¿›è¡Œè´¨é‡æ£€æŸ¥
            is_valid, error_msg = check_answer_quality(txt, question)
            
            if is_valid:
                print(f"    âœ… ç­”æ¡ˆè´¨é‡æ£€æŸ¥é€šè¿‡ï¼ˆé•¿åº¦: {len(txt)}å­—ç¬¦ï¼‰")
                return txt
            else:
                print(f"    âš ï¸ ç¬¬ {i+1} æ¬¡å°è¯•è´¨é‡æ£€æŸ¥å¤±è´¥: {error_msg}")
                if i < retry - 1:  # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                    print(f"    ğŸ”„ å°†é‡è¯•...")
                    time.sleep(pause)
                    continue
                else:
                    print(f"    âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä»è¿”å›å½“å‰ç­”æ¡ˆ")
                    return txt  # å³ä½¿è´¨é‡ä¸ä½³ä¹Ÿè¿”å›ï¼Œé¿å…å®Œå…¨å¤±è´¥
                    
        except Exception as e:
            print(f"âŒ {model} ç¬¬ {i+1} æ¬¡APIè°ƒç”¨å¤±è´¥: {e}")
            if i < retry - 1:
                time.sleep(pause)
    
    return ""

def load_progress(file: Path):
    if not file.exists():
        return {}
    try:
        with file.open("r", encoding="utf-8") as f:
            data = json.load(f)
        return {row["question"]: row for row in data}
    except Exception as e:
        print(f"âš ï¸ è¯»å–è¿›åº¦å¤±è´¥: {e}")
        return {}

def save_progress(done_dict: dict, file: Path):
    """ä¿å­˜è¿›åº¦ï¼Œåªä¿å­˜ done å­—å…¸ä¸­çš„è®°å½•"""
    try:
        rows = list(done_dict.values())
        tmp = file.with_suffix(".tmp")
        tmp.write_text(json.dumps(rows, ensure_ascii=False, indent=2), "utf-8")
        tmp.replace(file)
        print(f"ğŸ’¾ ä¿å­˜ {file.name} ï¼ˆ{len(rows)} æ¡ï¼‰")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

def load_answer_data(answer_path: Path):
    """è¯»å–åŒ…å«é—®é¢˜ã€combination_1_reply å’Œ third_answer çš„ JSON æ–‡ä»¶"""
    q2data = {}
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not answer_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {answer_path}")
        return q2data
        
    with answer_path.open("r", encoding="utf-8") as f:
        # åˆ¤æ–­æ–‡ä»¶æ˜¯ JSON æ•°ç»„è¿˜æ˜¯ JSON lines
        first_char = f.read(1)
        f.seek(0)
        if first_char == "[":
            # JSON æ•°ç»„
            entries = json.load(f)
        else:
            # JSON lines
            entries = [json.loads(line) for line in f if line.strip()]
    
    print(f"ğŸ“– è¯»å–åˆ° {len(entries)} æ¡è®°å½•")
    
    for i, entry in enumerate(entries):
        try:
            q = entry["question"]
            combination_reply = entry.get("combination_1_reply", "")
            third_answer = entry.get("third_answer", "")
            third_model = entry.get("third_model", "")
            
            # è°ƒè¯•ä¿¡æ¯
            if i == 0:  # åªæ‰“å°ç¬¬ä¸€æ¡è®°å½•çš„å­—æ®µä¿¡æ¯
                print(f"ğŸ” ç¬¬ä¸€æ¡è®°å½•çš„å­—æ®µ: {list(entry.keys())}")
                if "third_answer" in entry:
                    print(f"   âœ“ æ‰¾åˆ° third_answer (é•¿åº¦: {len(third_answer)})")
                else:
                    print(f"   âŒ æœªæ‰¾åˆ° third_answer")
                if "combination_1_reply" in entry:
                    print(f"   âœ“ æ‰¾åˆ° combination_1_reply (é•¿åº¦: {len(combination_reply)})")
                else:
                    print(f"   âŒ æœªæ‰¾åˆ° combination_1_reply")
            
            # åªæœ‰å½“ä¸¤ä¸ªç­”æ¡ˆéƒ½å­˜åœ¨æ—¶æ‰åŠ å…¥
            if combination_reply and third_answer:
                q2data[q] = {
                    "combination_reply": combination_reply,  # A2
                    "third_answer": third_answer,  # A1
                    "third_model": third_model
                }
            else:
                if i < 3:  # åªæ‰“å°å‰å‡ æ¡çš„è­¦å‘Šä¿¡æ¯
                    print(f"âš ï¸ ç¬¬ {i+1} æ¡è®°å½•ç¼ºå°‘å¿…è¦å­—æ®µ: combination_reply={bool(combination_reply)}, third_answer={bool(third_answer)}")
                    
        except KeyError as e:
            print(f"âš ï¸ ç¬¬ {i+1} æ¡è®°å½•ç¼ºå°‘å­—æ®µ {e}: {list(entry.keys())}")
            continue
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(q2data)} ä¸ªé—®é¢˜çš„æ•°æ®")
    return q2data

# ===== 5. è„šæœ¬å…¥å£ ==========================================================
if __name__ == "__main__":
    # 0) æ£€æŸ¥æ¨¡æ¿æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not PROMPT_FILE.exists():
        print(f"âš ï¸ æç¤ºè¯æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {PROMPT_FILE}")
        print("ğŸ“ è¯·åˆ›å»º prompt-2+1-2.txt æ–‡ä»¶ï¼ŒåŒ…å« {q}ã€{A1} å’Œ {A2} å ä½ç¬¦")
    else:
        print(f"âœ… å·²åŠ è½½æç¤ºè¯æ¨¡æ¿: {PROMPT_FILE}")
        print(f"ğŸ“‹ ç­”æ¡ˆè´¨é‡æ£€æŸ¥é…ç½®: æœ€å°é•¿åº¦={MIN_ANSWER_LENGTH}, æœ€å¤§é‡è¯•={MAX_RETRIES}")
    
    # 1) è¯»å–ç­”æ¡ˆæ•°æ®
    q2data = load_answer_data(ANSWER_FILE)
    all_questions = sorted(q2data.keys())
    print(f"ğŸ“š é¢˜ç›®æ•°: {len(all_questions)}")
    
    # 2) ä¸ºæ¯ä¸ªæ¨¡å‹å‡†å¤‡ APIã€è¿›åº¦æ–‡ä»¶ã€è¡Œç¼“å­˜
    model_env = {}
    for cfg in MODEL_CFGS:
        name = cfg["model_name"]
        output_filename = get_output_filename(name, cfg)
        output_path = OUTPUT_DIR / output_filename
        
        model_env[name] = {
            "api": OpenAI(api_key=cfg["api_key"], base_url=cfg["base_url"]),
            "out": output_path,
            "done": load_progress(output_path),
        }
        # æ‰“å°å·²æœ‰è¿›åº¦
        existing_count = len(model_env[name]["done"])
        if existing_count > 0:
            print(f"ğŸ“Š {name} å·²æœ‰è¿›åº¦: {existing_count} é¢˜")
        print(f"ğŸ“„ {name} è¾“å‡ºæ–‡ä»¶: {output_filename}")
    
    processed = 0
    skipped = 0
    quality_failures = 0
    
    # ------- ä¸»å¾ªç¯ï¼šé¢˜ç›®ä¼˜å…ˆ -----------------
    for qi, q in enumerate(all_questions, 1):
        print(f"\nğŸ“ [{qi}/{len(all_questions)}] {q[:60]}â€¦")
        
        # è·å–è¯¥é—®é¢˜çš„æ•°æ®
        data = q2data[q]
        a1 = data["third_answer"]  # third_answer ä½œä¸º A1
        a2 = data["combination_reply"]  # combination_1_reply ä½œä¸º A2
        source_model = data["third_model"]
        
        question_processed = False
        
        for cfg in MODEL_CFGS:
            mname = cfg["model_name"]
            env = model_env[mname]
            
            # å·²æœ‰åˆ™è·³è¿‡
            if q in env["done"]:
                print(f"  â­ï¸ {mname} å·²å¤„ç†è¿‡ï¼Œè·³è¿‡")
                skipped += 1
                continue
            
            api = env["api"]
            
            print(f"  ğŸ¤– è°ƒç”¨ {mname}")
            
            # ä½¿ç”¨æ–°æ¨¡æ¿æ„å»º prompt
            prompt = PROMPT_TEMPLATE.format(q=q, A1=a1, A2=a2)
            
            # è°ƒç”¨æ¨¡å‹ï¼ˆå·²åŒ…å«è´¨é‡æ£€æŸ¥ï¼‰
            reply = ask(api, mname, prompt, question=q)
            
            # è®°å½•è´¨é‡æ£€æŸ¥ç»“æœ
            if reply:
                is_valid, quality_msg = check_answer_quality(reply, q)
                if not is_valid:
                    quality_failures += 1
                    print(f"    âš ï¸ æœ€ç»ˆç­”æ¡ˆè´¨é‡é—®é¢˜: {quality_msg}")
            
            # ä¿å­˜ç»“æœ
            item = {
                "question": q,
                "third_model": source_model,
                "A1_third_answer": a1,
                "A2_combination_reply": a2,
                "fusion_prompt": prompt,
                "fusion_reply": reply,
                "quality_check": check_answer_quality(reply, q)[1] if reply else "ç”Ÿæˆå¤±è´¥"
            }
            
            # ç›´æ¥åŠ å…¥ done å­—å…¸ï¼Œä¸ä½¿ç”¨ rows
            env["done"][q] = item
            question_processed = True
        
        if question_processed:
            processed += 1
            
        # ---- SAVE_INTERVAL ----
        if processed > 0 and processed % SAVE_INTERVAL == 0:
            print(f"\nğŸ’¾ è¾¾åˆ°ä¿å­˜é—´éš”ï¼Œä¿å­˜è¿›åº¦...")
            for mname, env in model_env.items():
                save_progress(env["done"], env["out"])
    
    # 3) å…¨éƒ¨å®Œæˆåä¿å­˜ä¸€æ¬¡
    print(f"\nğŸ å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ–°å¤„ç†: {processed} é¢˜")
    print(f"   - è·³è¿‡: {skipped} é¢˜") 
    print(f"   - è´¨é‡é—®é¢˜: {quality_failures} é¢˜")
    
    for mname, env in model_env.items():
        save_progress(env["done"], env["out"])
        print(f"âœ… {mname} æ€»è®¡ {len(env['done'])} æ¡è®°å½•")
    
    print(f"\nğŸ‰ æŒ‰é¢˜ç›®é¡ºåºå…¨éƒ¨å¤„ç†å®Œæ¯•ï¼Œæ–‡ä»¶ä¿å­˜åœ¨: {OUTPUT_DIR}")