#!/usr/bin/env python
# coding: utf-8
"""
generate_multi_answers.py
-------------------------
è¯»å– generated_results_multi_model.jsonï¼Œ
å¯¹æ¯ä¸ªæ¨¡å‹ / æ¯ä¸ªæ ¸å¿ƒé—®é¢˜ç”Ÿæˆï¼ˆæˆ–å¤ç”¨ï¼‰ basic_answer å’Œ
answer_with_contextï¼Œå¹¶ä¿å­˜åˆ° final_ans_multi_sm.jsonã€‚

è‹¥ grouped_answers.json ä¸­å·²å­˜åœ¨å¯¹åº” basic_answerï¼Œåˆ™ç›´æ¥å¤ç”¨ï¼Œ
é¿å…é‡å¤ç”Ÿæˆï¼›åªæœ‰ç¼ºå¤±æ—¶æ‰ä¼šè°ƒç”¨ APIã€‚
"""

import os
import json
import time
from pathlib import Path
from typing import Dict, Any
from openai import OpenAI
import httpx


# ========= è·¯å¾„ & å¸¸é‡ ========================================================
INPUT_FILE     = Path(r"D:\project\generated_results_multi_model.json")
BASIC_ANS_FILE = Path(r"D:\project\grouped_answers.json")           # â† å·²æœ‰ basic ç­”æ¡ˆ
OUTPUT_DIR     = Path(r"D:\project")
OUTPUT_FILE    = OUTPUT_DIR / "final_ans_multi_sm.json"


# ========= æ¨¡å‹é…ç½®ï¼ˆå®Œæ•´ï¼‰ ===================================================
models_config: Dict[str, Dict[str, str]] = {
    "gemini-2.5-flash-preview-04-17-thinking": {
        "api_key": "sk-VJrRRrYljSfcLQPKD2ocOw8NrKaFOPsTszZy1gb5qWJixq2Y",
        "base_url": "https://api.aigptapi.com/v1/"
    },
    "grok-3-beta": {
        "api_key": "sk-VJrRRrYljSfcLQPKD2ocOw8NrKaFOPsTszZy1gb5qWJixq2Y",
        "base_url": "https://api.aigptapi.com/v1/"
    },
    "doubao-pro-256k": {
        "api_key": "sk-N4rH9BjW8xR1akf0C01426F958D74c9d97Bd7a131a09B5B4",
        "base_url": "https://api.vansai.cn/v1"
    },
    "moonshot-v1-8k": {
        "api_key": "sk-N4rH9BjW8xR1akf0C01426F958D74c9d97Bd7a131a09B5B4",
        "base_url": "https://api.vansai.cn/v1"
    },
    "deepseek-v3": {
        "api_key": "sk-N4rH9BjW8xR1akf0C01426F958D74c9d97Bd7a131a09B5B4",
        "base_url": "https://api.vansai.cn/v1"
    },
    "hunyuan-turbo": {
        "api_key": "sk-N4rH9BjW8xR1akf0C01426F958D74c9d97Bd7a131a09B5B4",
        "base_url": "https://api.vansai.cn/v1"
    },
    "qwen2.5-72b-instruct": {
        "api_key": "sk-N4rH9BjW8xR1akf0C01426F958D74c9d97Bd7a131a09B5B4",
        "base_url": "https://api.vansai.cn/v1"
    }
}


# ========= å·¥å…·å‡½æ•° ===========================================================
def load_basic_answer_map(filepath: Path) -> Dict[str, Dict[str, str]]:
    """è¯»å– grouped_answers.jsonï¼Œè¿”å›æ˜ å°„ï¼š{æ ¸å¿ƒé—®é¢˜: {model_name: basic_answer, ...}, ...}"""
    if not filepath.exists():
        return {}

    with filepath.open("r", encoding="utf-8") as f:
        raw: Dict[str, Any] = json.load(f)

    q2model2ans: Dict[str, Dict[str, str]] = {}
    for core_q, blocks in raw.items():
        basic_list = blocks.get("basic_answers", [])
        for model_name, answer in basic_list:
            q2model2ans.setdefault(core_q, {})[model_name] = answer
    return q2model2ans


def get_completion(client: OpenAI,
                   model_name: str,
                   prompt: str,
                   attempt: int = 1,
                   max_attempts: int = 10) -> str:
    """å¸¦é‡è¯•çš„ OpenAI API è¯·æ±‚ã€‚"""
    try:
        rsp = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}]
        )
        return rsp.choices[0].message.content
    except Exception as e:
        if attempt < max_attempts:
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥ï¼Œé‡è¯•ä¸­ ({attempt}/{max_attempts})... åŸå› ï¼š{e}")
            time.sleep(2)
            return get_completion(client, model_name, prompt,
                                  attempt + 1, max_attempts)
        print(f"âŒ æœ€ç»ˆå¤±è´¥ï¼š{e}")
        return f"ERROR: {str(e)}"


# ========= ä¸»æµç¨‹ =============================================================
def main() -> None:
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    with INPUT_FILE.open("r", encoding="utf-8") as f:
        raw_data = json.load(f)

    basic_answer_map = load_basic_answer_map(BASIC_ANS_FILE)
    all_results = []

    for model_block in raw_data:
        model_name = model_block["model_name"]
        print(f"\n{'='*60}\nğŸ” å¼€å§‹å¤„ç†æ¨¡å‹ï¼š{model_name}\n{'='*60}")

        config = models_config.get(model_name)
        if not config:
            print(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹é…ç½®ï¼Œè·³è¿‡ï¼š{model_name}")
            continue

        client = OpenAI(
            api_key=config["api_key"],
            base_url=config["base_url"],
            http_client=httpx.Client(verify=False)
        )

        model_results = []

        for entry in model_block["results"]:
            core_q_data = entry.get("core_question", [])
            if not core_q_data or not isinstance(core_q_data, list):
                continue

            core_question = core_q_data[0]
            sum_list_data = entry.get("sum_list", [])
            print(f"\nâ¡ï¸ é—®é¢˜ï¼š{core_question}")

            # -------- æ„é€  context ä¿¡æ¯ï¼ˆè·³è¿‡è¶…è¿‡30å­—ç¬¦çš„éƒ¨åˆ†ï¼‰ --------
            context_parts = []
            for grp in sum_list_data:
                if not grp or not isinstance(grp, list) or len(grp) < 2:
                    continue
                part = f"{grp[0]}é¢†åŸŸï¼š{'ã€'.join(grp[1:])}"
                if len(part) > 30:
                    print(f"âš ï¸ è·³è¿‡è¿‡é•¿å› ç´ ï¼š{part}")
                    continue
                context_parts.append(part)
            context_str = "ï¼›".join(context_parts)

            # -------- 1. basic answer --------
            basic_ans = basic_answer_map.get(core_question, {}).get(model_name)
            if basic_ans is None:
                prompt_basic = f"è¯·å›ç­”ï¼š\"{core_question}\"ã€‚"
                basic_ans = get_completion(client, model_name, prompt_basic)
                print("    ï¼ˆæœªæ‰¾åˆ°ç°æˆ basic_answerï¼Œå·²è°ƒç”¨ API è¡¥å……ï¼‰")

            # -------- 2. answer_with_context --------
            prompt_context = (f"è¯·å›ç­”ï¼š\"{core_question}\"ï¼Œç»“åˆä»¥ä¸‹ä¿¡æ¯ï¼Œ"
                              f"å‚è€ƒä»¥ä¸‹é‡è¦å› ç´ è¿›è¡Œä½œç­”ï¼Œå› ç´ æŒ‰é‡è¦æ€§æ’åºï¼š{context_str}ã€‚")
            context_ans = get_completion(client, model_name, prompt_context)

            # -------- è®°å½•ç»“æœ --------
            model_results.append({
                "core_question": core_question,
                "basic_answer": basic_ans,
                "answer_with_context": context_ans,
                "context_info": context_str
            })

        all_results.append({
            "model_name": model_name,
            "results": model_results
        })
        print(f"\nâœ… æ¨¡å‹ {model_name} å…¨éƒ¨é—®é¢˜å¤„ç†å®Œæˆï¼")

    # -------- ä¿å­˜åˆ°æ–‡ä»¶ --------
    with OUTPUT_FILE.open("w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=4)

    print(f"\nğŸ‰ æ‰€æœ‰æ¨¡å‹å¤„ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³ï¼š{OUTPUT_FILE}")


# ========= å…¥å£ ==============================================================
if __name__ == "__main__":
    main()
