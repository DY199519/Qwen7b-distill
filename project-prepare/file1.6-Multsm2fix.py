#éåŽ†ä¸€éä¸Šé¢çš„æ¨¡åž‹ï¼Œå¡«è¡¥ç½‘ç»œé”™è¯¯æ²¡æœ‰ç­”æ¡ˆçš„é—®é¢˜

import os, json, time, datetime
from pathlib import Path

import httpx
from openai import OpenAI

###############################################################################
# 0. æ¨¡åž‹æŽ¥å£é…ç½®
###############################################################################
models_config = {
    "gemini-2.5-flash-preview-04-17-thinking": {
        "api_key": "sk-VJrRRrYljSfcLQPKD2ocOw8NrKaFOPsTszZy1gb5qWJixq2Y",
        "base_url": "https://api.aigptapi.com/v1/"
    },
    "grok-3-beta": {
        "api_key": "sk-VJrRRrYljSfcLQPKD2ocOw8NrKaFOPsTszZy1gb5qWJixq2Y",
        "base_url": "https://api.aigptapi.com/v1/"
    },
    # "qwen3-235b-a22b": {
    #     "api_key": "sk-N4rH9BjW8xR1akf0C01426F958D74c9d97Bd7a131a09B5B4",
    #     "base_url": "https://api.vansai.cn/v1"
    # },
    "doubao-pro-256k": {
        "api_key": "sk-N4rH9BjW8xR1akf0C01426F958D74c9d97Bd7a131a09B5B4",
        "base_url": "https://api.vansai.cn/v1"
    },
    "moonshot-v1-8k": {
        "api_key": "sk-N4rH9BjW8xR1akf0C01426F958D74c9d97Bd7a131a09B5B4",
        "base_url": "https://api.vansai.cn/v1"
    },
    "deepseek-v3": {
        "api_key": "sk-N4rH9BjW8xR1akf0C01426F958D74c9d97Bd7a131a09B5B4",
        "base_url": "https://api.vansai.cn/v1"
    },
    "hunyuan-turbo": {
        "api_key": "sk-N4rH9BjW8xR1akf0C01426F958D74c9d97Bd7a131a09B5B4",
        "base_url": "https://api.vansai.cn/v1"
    },
    "qwen2.5-72b-instruct": {
        "api_key": "sk-N4rH9BjW8xR1akf0C01426F958D74c9d97Bd7a131a09B5B4",
        "base_url": "https://api.vansai.cn/v1"
    }
}

###############################################################################
# 1. è¾“å…¥/è¾“å‡ºè·¯å¾„
###############################################################################
input_file  = r"D:\project\final_ans_multi_sm.json"
output_dir  = r"D:\project"

os.makedirs(output_dir, exist_ok=True)                 # ç¡®ä¿ç›®å½•å­˜åœ¨
in_path  = Path(input_file)
if not in_path.exists():
    raise FileNotFoundError(f"æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ï¼š{in_path}")

###############################################################################
# 2. è¾…åŠ©å‡½æ•°
###############################################################################
def contains_error(text: str | None) -> bool:
    return isinstance(text, str) and "error" in text.lower()

def get_completion(client: OpenAI, model: str, prompt: str,
                   attempt: int = 1, max_attempts: int = 10) -> str:
    try:
        rsp = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        return rsp.choices[0].message.content
    except Exception as e:
        if attempt < max_attempts:
            print(f"âš ï¸ è¯·æ±‚å¤±è´¥ï¼Œé‡è¯•ä¸­ ({attempt}/{max_attempts})... åŽŸå› ï¼š{e}")
            time.sleep(2)
            return get_completion(client, model, prompt, attempt + 1, max_attempts)
        print(f"âŒ æœ€ç»ˆå¤±è´¥ï¼š{e}")
        return f"ERROR: {e}"

###############################################################################
# 3. è¯»å…¥æ•°æ®
###############################################################################
with in_path.open(encoding="utf-8") as f:
    raw_data = json.load(f)

if not isinstance(raw_data, list):
    raise ValueError("è¾“å…¥æ–‡ä»¶é¡¶å±‚åº”æ˜¯æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ªæ¨¡åž‹å—ã€‚")

processed_all = []

###############################################################################
# 4. æŒ‰æ¨¡åž‹éåŽ†
###############################################################################
for model_block in raw_data:
    model_name = model_block.get("model_name", "").strip()
    print(f"\n{'='*60}\nðŸ” å¤„ç†æ¨¡åž‹ï¼š{model_name}\n{'='*60}")

    cfg = models_config.get(model_name)
    if not cfg:
        print(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡åž‹é…ç½®ï¼Œè·³è¿‡ï¼š{model_name}")
        continue

    client = OpenAI(
        api_key    = cfg["api_key"],
        base_url   = cfg["base_url"],
        http_client= httpx.Client(verify=False)   # ç¤ºä¾‹æœåŠ¡è¯ä¹¦é—®é¢˜
    )

    for item in model_block.get("results", []):
        q = item.get("core_question", "").strip()
        ctx = item.get("context_info", "").strip()

        # basic_answer
        if contains_error(item.get("basic_answer")):
            prompt_basic = f'è¯·å›žç­”ï¼šâ€œ{q}â€ã€‚'
            print(f"ðŸ”„ basic_answer: {q}")
            item["basic_answer"] = get_completion(client, model_name, prompt_basic)

        # answer_with_context
        if contains_error(item.get("answer_with_context")):
            prompt_ctx = (
                f'è¯·å›žç­”ï¼šâ€œ{q}â€ï¼Œç»“åˆä»¥ä¸‹ä¿¡æ¯ï¼ŒæŒ‰é‡è¦æ€§é¡ºåºä½œç­”ï¼Œ'
                f'å› ç´ æŒ‰é‡è¦æ€§æŽ’åºï¼š{ctx}ã€‚'
            )
            print(f"ðŸ”„ answer_with_context: {q}")
            item["answer_with_context"] = get_completion(client, model_name, prompt_ctx)

    # å•æ¨¡åž‹è¾“å‡º
    model_fixed = Path(output_dir) / f"{model_name}_fixed.json"
    with model_fixed.open("w", encoding="utf-8") as f:
        json.dump(model_block, f, ensure_ascii=False, indent=4)
    print(f"âœ… å·²ä¿å­˜ï¼š{model_fixed}")

    processed_all.append(model_block)

###############################################################################
# 5. æ±‡æ€»è¾“å‡º
###############################################################################
all_fixed = Path(output_dir) / "finalmut_fixed_all.json"
with all_fixed.open("w", encoding="utf-8") as f:
    json.dump(processed_all, f, ensure_ascii=False, indent=4)

print(f"\nðŸŽ‰ å…¨éƒ¨å®Œæˆï¼Œæ±‡æ€»æ–‡ä»¶ï¼š{all_fixed.resolve()}")
