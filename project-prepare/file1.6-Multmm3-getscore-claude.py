#!/usr/bin/env python
# coding: utf-8
"""
pairwise_grade_top234.py
------------------------
ä» `claude_combined_answers.json` è¯»å–æ¯ä¸ªé—®é¢˜çš„ top2 / top3 / top4 / direct å›ç­”ï¼Œ
ä¸¤ä¸¤é…å¯¹ï¼ˆå…± 6 ç»„ï¼‰ï¼Œè°ƒç”¨ gpt-4o è¿›è¡Œ 5Ã—100 æ‰“åˆ†å¹¶åˆ¤å®šèƒœè´Ÿã€‚
"""

import json, itertools, re, os, time
from pathlib import Path
from typing import List, Dict, Any, Tuple
from openai import OpenAI
import httpx
from tqdm import tqdm

# ========== OpenAI åˆå§‹åŒ– ====================================================
httpx_client = httpx.Client(verify=False)
os.environ["OPENAI_API_KEY"] = "sk-gwwbtmiiMKmF9h3P858dCaC14dB94bCc9bD728BaA6Bf082d"
os.environ["OPENAI_BASE_URL"] = "https://api.vansai.cn/v1"
client = OpenAI(http_client=httpx_client)

# ========== è·¯å¾„è®¾ç½® ========================================================
INPUT_PATH  = r"D:\project\claude_combined_answers.json"
OUTPUT_DIR  = r"D:\project"
OUTPUT_NAME = "pairwise_grades_top234_claude.json"

# ========== Prompt æ¨¡æ¿ =====================================================
PROMPT_TMPL = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šç­”é¢˜è¯„å®¡å‘˜ï¼Œè¯·å¯¹ä¸¤ä¸ªç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒ æŒ‰ç…§ä»¥ä¸‹ 5 ä¸ªç»´åº¦ç»™æ¯ä¸ªç­”æ¡ˆæ‰“åˆ†ï¼š 
1. é€»è¾‘æ€§   2. æ·±åº¦   3. åˆ›æ–°æ€§   4. å‡†ç¡®æ€§   5. å®Œæ•´æ€§
æ¯ç»´åº¦æ»¡åˆ† 100ï¼Œæ€»åˆ† 500ã€‚  

### æ ¸å¿ƒé—®é¢˜
{core_question}

### å›ç­”
A:
{answer_a}

B:
{answer_b}

### è¾“å‡ºè¦æ±‚
- å…ˆè¾“å‡º 2 è¡Œåˆ†æ•°ï¼Œæ¯è¡Œå¯¹åº” Aã€Bï¼Œæ ¼å¼ï¼šæ€»åˆ† é€»è¾‘ æ·±åº¦ åˆ›æ–° å‡†ç¡® å®Œæ•´ ï¼ˆä»…æ•°å­—ã€ç©ºæ ¼ï¼‰
- æ¥ç€å•ç‹¬ä¸€è¡Œè¾“å‡ºèƒœè´Ÿï¼Œå†…å®¹ä¸º AB æˆ– BA
- æœ€åä¸€æ®µç»™å‡ºè¯„åˆ†ç†ç”±å¹¶å¼•ç”¨ä¾æ®
ä¸¥æ ¼éµå®ˆæ ¼å¼ï¼Œç°åœ¨å¼€å§‹ï¼š
"""

# ========== è§£æ GPT å“åº” ===================================================
def parse_response(raw: str) -> Tuple[dict, dict, List[str], str]:
    keys = ["total", "logic", "depth", "innovation", "accuracy", "completeness"]
    lines = [l.strip() for l in raw.splitlines() if l.strip()]
    score_rows = []

    for l in lines:
        nums = re.findall(r"\d+", l)
        if len(nums) >= 6:
            score_rows.append([int(n) for n in nums[:6]])
        if len(score_rows) == 2:
            break
    if len(score_rows) != 2:
        raise ValueError("æ‰¾ä¸åˆ°ä¸¤è¡Œå®Œæ•´åˆ†æ•°")

    winner_line = next((l for l in lines if re.fullmatch(r"[ABab]{2}", l)), "")
    if not winner_line:
        raise ValueError("æœªæ‰¾åˆ°èƒœè´Ÿè¡Œ AB/BA")
    win_idx = lines.index(winner_line)
    commentary = "\n".join(lines[win_idx + 1:]).strip()

    return (
        dict(zip(keys, score_rows[0])),
        dict(zip(keys, score_rows[1])),
        list(winner_line.upper()),
        commentary
    )

# ========== GPT æ‰“åˆ†å¹¶è‡ªåŠ¨é‡è¯• ===============================================
def ask_and_parse(prompt: str,
                  model: str = "gpt-4o",
                  max_attempts: int = 6,
                  backoff_base: int = 2):
    for attempt in range(1, max_attempts + 1):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            raw = resp.choices[0].message.content.strip()
            parsed = parse_response(raw)
            return *parsed, raw
        except Exception as e:
            wait = backoff_base ** attempt
            print(f"âš ï¸ å°è¯• {attempt}/{max_attempts} å¤±è´¥ï¼š{e} â€”â€” {wait}s later retry")
            time.sleep(wait)
    raise RuntimeError("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ä»æœªè·å¾—åˆè§„å›ç­”")

# ========== å•ä¸ªé…å¯¹è¯„åˆ† =====================================================
def grade_pair(core_q: str, model_a: str, text_a: str, model_b: str, text_b: str):
    prompt = PROMPT_TMPL.format(core_question=core_q, answer_a=text_a, answer_b=text_b)
    print("\n" + "-"*60)
    print(f"Q: {core_q[:80]}...")
    print(f"A by {model_a} | B by {model_b}")
    scores_a, scores_b, winner, commentary, raw = ask_and_parse(prompt)
    return {
        "model_a": model_a,
        "model_b": model_b,
        "scores_a": scores_a,
        "scores_b": scores_b,
        "winner_order": winner,
        "commentary": commentary,
        "gpt_raw": raw[:800] + "..." if len(raw) > 800 else raw
    }

# ========== ä¸»æµç¨‹ ===========================================================
def main():
    with open(INPUT_PATH, "r", encoding="utf-8") as f:
        items = json.load(f)

    results = {}

    # åŠ¨æ€ç»Ÿè®¡æ€»é…å¯¹æ•°
    total_pairs = 0
    valid_items = []

    for entry in items:
        candidates = [
            ("top2", entry.get("top2_reply", "")),
            ("top3", entry.get("top3_reply", "")),
            ("top4", entry.get("top4_reply", "")),
            ("direct", entry.get("direct_reply", ""))  # âœ… ä¿®å¤å­—æ®µå
        ]
        answers = [(name, text.strip()) for name, text in candidates if text and text.strip()]
        if len(answers) >= 2:
            valid_items.append((entry, answers))
            total_pairs += len(answers) * (len(answers) - 1) // 2

    print(f"ğŸŒŸ å…± {len(valid_items)} é“é¢˜è¿›å…¥é…å¯¹è¯„åˆ†ï¼Œæ€»è®¡ {total_pairs} ä¸ªé…å¯¹")

    done = 0
    with tqdm(total=total_pairs, desc="é…å¯¹è¯„åˆ†è¿›åº¦") as pbar:
        for entry, answers in valid_items:
            question = entry.get("question") or entry.get("direct_prompt")
            ans_pairs = []

            # ä¼˜å…ˆå¤„ç† direct vs others
            direct_item = next(((name, text) for name, text in answers if name == "direct"), None)
            others = [(name, text) for name, text in answers if name != "direct"]

            if direct_item:
                for other in others:
                    m1, t1 = direct_item
                    m2, t2 = other
                    print(f"ğŸ” ä¼˜å…ˆå¤„ç†ï¼š{m1} vs {m2}")
                    ans_pairs.append(grade_pair(question, m1, t1, m2, t2))
                    done += 1
                    pbar.update(1)

            for (m1, t1), (m2, t2) in itertools.combinations(others, 2):
                print(f"ğŸ”„ å¸¸è§„å¤„ç†ï¼š{m1} vs {m2}")
                ans_pairs.append(grade_pair(question, m1, t1, m2, t2))
                done += 1
                pbar.update(1)

            results[question] = {"top_pairs": ans_pairs}

    Path(OUTPUT_DIR).mkdir(exist_ok=True, parents=True)
    out_path = Path(OUTPUT_DIR) / OUTPUT_NAME
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼Œ{done} ä¸ªé…å¯¹å·²è¯„åˆ†ï¼Œç»“æœå†™å…¥ï¼š{out_path}")

# ========== å¯åŠ¨ ============================================================
if __name__ == "__main__":
    main()
