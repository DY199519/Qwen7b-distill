#!/usr/bin/env python
# coding: utf-8
"""
multmm1_build_prompts_txt.py
----------------------------
è¯»å–JSONæ–‡ä»¶å¹¶ç”Ÿæˆpromptï¼š
  Â· è¯»å–åŒ…å«questionså’Œanswersçš„JSONæ–‡ä»¶
  Â· æ¨¡ç³ŠåŒ¹é…åŒ…å«gemini, grok, doubaoçš„æ¨¡å‹
  Â· ä»å¤–éƒ¨æ–‡ä»¶è¯»å–promptæ¨¡æ¿
  Â· ç”Ÿæˆpromptå¹¶å†™å…¥TXTæ–‡ä»¶ï¼Œæ¯ä¸ªpromptä¸ºä¸€æ®µ
  Â· æ·»åŠ çº é”™æœºåˆ¶ï¼Œæ£€æŸ¥ç­”æ¡ˆçš„æœ‰æ•ˆæ€§
"""

import json
from pathlib import Path
import re
from typing import Dict, List, Tuple, Optional

# === 1. è·¯å¾„é…ç½® ===
BASE_DIR = Path(r"D:\project7\prompt")
BASE_DIR_1 = Path(r"D:\project7")
BASE_DIR_2 = Path(r"D:\project7\MM\result")

# ä½¿ç”¨ç¬¬äºŒä¸ªJSONæ–‡ä»¶
json_path = BASE_DIR_1 / "multi_model_answers9400-10000.json"

# Prompt æ–‡ä»¶è·¯å¾„
PROMPT_FILE = BASE_DIR_2 / "prompt-3+1withoutsummary.txt"

# è¾“å‡ºæ–‡ä»¶ï¼ˆæ”¹ä¸ºtxtï¼‰
OUT_TXT = BASE_DIR_2 / "final_prompt_3+1-Test.txt"
ERROR_LOG = BASE_DIR_2 / "error_log.txt"  # é”™è¯¯æ—¥å¿—æ–‡ä»¶

# === 2. çº é”™é…ç½® ===
class ErrorChecker:
    """ç­”æ¡ˆçº é”™æ£€æŸ¥å™¨"""
    
    # æœ€å°ç­”æ¡ˆé•¿åº¦
    MIN_ANSWER_LENGTH = 10
    
    # æœ€å¤§ç­”æ¡ˆé•¿åº¦ï¼ˆå¯èƒ½æ˜¯é”™è¯¯ï¼‰
    MAX_ANSWER_LENGTH = 10000
    
    # å¸¸è§é”™è¯¯æ¨¡å¼
    ERROR_PATTERNS = [
        r'^error:',  # ä»¥errorå¼€å¤´
        r'^exception:',  # ä»¥exceptionå¼€å¤´
        r'^\s*$',  # çº¯ç©ºç™½
        r'^null$',  # nullå€¼
        r'^undefined$',  # undefinedå€¼
        r'^N/A$',  # N/A
        r'^\[.*error.*\]$',  # åŒ…å«errorçš„æ–¹æ‹¬å·å†…å®¹
        r'^\{.*error.*\}$',  # åŒ…å«errorçš„èŠ±æ‹¬å·å†…å®¹
    ]
    
    # å¯ç–‘æ¨¡å¼ï¼ˆè­¦å‘Šä½†ä¸è¿‡æ»¤ï¼‰
    WARNING_PATTERNS = [
        r'^.{1,9}$',  # è¿‡çŸ­çš„ç­”æ¡ˆï¼ˆå°äº10å­—ç¬¦ï¼‰
        r'^\d+$',  # çº¯æ•°å­—
        r'^[^\u4e00-\u9fa5a-zA-Z]+$',  # æ²¡æœ‰ä¸­æ–‡æˆ–è‹±æ–‡å­—æ¯
        r'(.)\1{10,}',  # é‡å¤å­—ç¬¦è¶…è¿‡10æ¬¡
    ]
    
    @classmethod
    def check_context_format(cls, context: str) -> Tuple[bool, List[str]]:
        """
        æ£€æŸ¥ç”Ÿæˆçš„ä¸Šä¸‹æ–‡æ ¼å¼æ˜¯å¦æ­£ç¡®
        ç¡®ä¿æ¯ä¸ª"å›ç­”Xï¼š"åé¢éƒ½æœ‰å®é™…å†…å®¹
        """
        issues = []
        
        # æå–æ‰€æœ‰å›ç­”éƒ¨åˆ†
        pattern = r'å›ç­”(\d+)ï¼š(.*?)(?=å›ç­”\d+ï¼š|$)'
        matches = re.findall(pattern, context, re.DOTALL)
        
        if not matches:
            issues.append("æœªæ‰¾åˆ°æ ‡å‡†çš„'å›ç­”Xï¼š'æ ¼å¼")
            return False, issues
        
        # æ£€æŸ¥æ¯ä¸ªå›ç­”
        for num, content in matches:
            content = content.strip()
            if not content:
                issues.append(f"å›ç­”{num}ä¸ºç©º")
            elif len(content) < cls.MIN_ANSWER_LENGTH:
                issues.append(f"å›ç­”{num}å†…å®¹è¿‡çŸ­ ({len(content)}å­—ç¬¦)")
        
        # æ£€æŸ¥å›ç­”ç¼–å·æ˜¯å¦è¿ç»­
        numbers = [int(num) for num, _ in matches]
        numbers.sort()
        expected = list(range(1, len(numbers) + 1))
        if numbers != expected:
            issues.append(f"å›ç­”ç¼–å·ä¸è¿ç»­: {numbers}")
        
        return len(issues) == 0, issues
    
    @classmethod
    def check_answer(cls, answer: str, question: str = "") -> Tuple[bool, str, List[str]]:
        """
        æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æœ‰æ•ˆ
        è¿”å›: (æ˜¯å¦æœ‰æ•ˆ, æ¸…ç†åçš„ç­”æ¡ˆ, é”™è¯¯/è­¦å‘Šåˆ—è¡¨)
        """
        errors = []
        warnings = []
        
        # åŸºæœ¬æ£€æŸ¥
        if not answer:
            errors.append("ç­”æ¡ˆä¸ºç©º")
            return False, "", errors
        
        # ç±»å‹æ£€æŸ¥
        if not isinstance(answer, str):
            errors.append(f"ç­”æ¡ˆç±»å‹é”™è¯¯: {type(answer)}")
            return False, "", errors
        
        # æ¸…ç†ç­”æ¡ˆï¼ˆå»é™¤é¦–å°¾ç©ºç™½ï¼‰
        cleaned_answer = answer.strip()
        
        # é•¿åº¦æ£€æŸ¥
        if len(cleaned_answer) < cls.MIN_ANSWER_LENGTH:
            warnings.append(f"ç­”æ¡ˆè¿‡çŸ­ ({len(cleaned_answer)} å­—ç¬¦)")
        elif len(cleaned_answer) > cls.MAX_ANSWER_LENGTH:
            warnings.append(f"ç­”æ¡ˆè¿‡é•¿ ({len(cleaned_answer)} å­—ç¬¦)")
        
        # é”™è¯¯æ¨¡å¼æ£€æŸ¥
        for pattern in cls.ERROR_PATTERNS:
            if re.match(pattern, cleaned_answer, re.IGNORECASE):
                errors.append(f"åŒ¹é…é”™è¯¯æ¨¡å¼: {pattern}")
                return False, cleaned_answer, errors
        
        # è­¦å‘Šæ¨¡å¼æ£€æŸ¥
        for pattern in cls.WARNING_PATTERNS:
            if re.match(pattern, cleaned_answer, re.IGNORECASE):
                warnings.append(f"åŒ¹é…å¯ç–‘æ¨¡å¼: {pattern}")
        
        # ç‰¹æ®Šå­—ç¬¦æ£€æŸ¥
        if cleaned_answer.count('\n') > 50:
            warnings.append("åŒ…å«è¿‡å¤šæ¢è¡Œç¬¦")
        
        if cleaned_answer.count(' ') / len(cleaned_answer) > 0.5:
            warnings.append("ç©ºæ ¼æ¯”ä¾‹è¿‡é«˜")
        
        # è¿”å›ç»“æœ
        is_valid = len(errors) == 0
        all_issues = errors + warnings
        
        return is_valid, cleaned_answer, all_issues

# === 3. è¯»å– Prompt æ¨¡æ¿ ===
def load_prompt_template():
    """ä»æ–‡ä»¶è¯»å– prompt æ¨¡æ¿"""
    try:
        with open(PROMPT_FILE, 'r', encoding='utf-8') as f:
            template = f.read().strip()
            print(f"âœ“ æˆåŠŸè¯»å– prompt æ¨¡æ¿ï¼š{PROMPT_FILE}")
            return template
    except FileNotFoundError:
        print(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ° prompt æ–‡ä»¶ï¼š{PROMPT_FILE}")
        # ä½¿ç”¨é»˜è®¤æ¨¡æ¿ä½œä¸ºå¤‡ä»½
        default_template = 'è¯·å›ç­”ï¼š"{q}"ï¼ŒåŸºäºä»¥ä¸‹å›ç­”å¯¹ä½ çš„ç­”æ¡ˆè¿›è¡Œå®Œå–„ï¼š{ctx}ã€‚'
        print(f"  ä½¿ç”¨é»˜è®¤ prompt æ¨¡æ¿")
        return default_template

# === 4. å·¥å…·å‡½æ•° ===
def extract_answers_fuzzy(question_data: Dict, question: str, error_log: List[Dict]) -> Tuple[List[str], List[str], Dict]:
    """
    æ¨¡ç³ŠåŒ¹é…åŒ…å« gemini, grok, doubao çš„æ¨¡å‹å¹¶æå–ç­”æ¡ˆ
    æ·»åŠ çº é”™æœºåˆ¶
    """
    answers = []
    found_models = []
    target_keywords = ['gemini', 'grok', 'doubao']
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_models': 0,
        'matched_models': 0,
        'valid_answers': 0,
        'invalid_answers': 0,
        'warnings': 0
    }
    
    if "answers" in question_data:
        for model_name, model_answers in question_data["answers"].items():
            stats['total_models'] += 1
            
            # æ¨¡ç³ŠåŒ¹é…ï¼šæ£€æŸ¥æ¨¡å‹åæ˜¯å¦åŒ…å«ç›®æ ‡å…³é”®è¯
            model_lower = model_name.lower()
            for keyword in target_keywords:
                if keyword in model_lower:
                    stats['matched_models'] += 1
                    
                    if model_answers and len(model_answers) > 0:
                        # åªå–ç¬¬ä¸€ä¸ªç­”æ¡ˆ
                        raw_answer = model_answers[0].get("answer", "")
                        
                        # çº é”™æ£€æŸ¥
                        is_valid, cleaned_answer, issues = ErrorChecker.check_answer(raw_answer, question)
                        
                        if is_valid:
                            if cleaned_answer:  # å†æ¬¡ç¡®è®¤æ¸…ç†åçš„ç­”æ¡ˆä¸ä¸ºç©º
                                answers.append(cleaned_answer)
                                found_models.append(model_name)
                                stats['valid_answers'] += 1
                                
                                # å¦‚æœæœ‰è­¦å‘Šï¼Œè®°å½•ä½†ä¸é˜»æ­¢ä½¿ç”¨
                                if issues:
                                    stats['warnings'] += 1
                                    error_log.append({
                                        'type': 'warning',
                                        'question': question[:50] + '...' if len(question) > 50 else question,
                                        'model': model_name,
                                        'issues': issues,
                                        'answer_preview': cleaned_answer[:50] + '...' if len(cleaned_answer) > 50 else cleaned_answer
                                    })
                        else:
                            stats['invalid_answers'] += 1
                            error_log.append({
                                'type': 'error',
                                'question': question[:50] + '...' if len(question) > 50 else question,
                                'model': model_name,
                                'issues': issues,
                                'raw_answer': raw_answer[:50] + '...' if len(raw_answer) > 50 else raw_answer
                            })
                    break  # æ‰¾åˆ°åŒ¹é…å°±è·³å‡ºå†…å±‚å¾ªç¯
    
    return answers, found_models, stats

def build_prompts(questions_data: Dict, prompt_template: str) -> Tuple[List[str], List[Dict]]:
    """æ„é€ promptåˆ—è¡¨ï¼Œè¿”å›promptåˆ—è¡¨å’Œé”™è¯¯æ—¥å¿—"""
    prompts = []
    error_log = []
    
    # å…¨å±€ç»Ÿè®¡
    global_stats = {
        'total_questions': 0,
        'matched_questions': 0,
        'skipped_questions': 0,
        'total_errors': 0,
        'total_warnings': 0,
        'context_format_errors': 0
    }
    
    print(f"\nğŸ“‹ å¼€å§‹å¤„ç†æ•°æ®...")
    
    for question, question_data in questions_data.items():
        global_stats['total_questions'] += 1
        
        # æå–æ¨¡ç³ŠåŒ¹é…çš„ç­”æ¡ˆï¼ˆå¸¦çº é”™ï¼‰
        answers, found_models, stats = extract_answers_fuzzy(question_data, question, error_log)
        
        # æ›´æ–°å…¨å±€ç»Ÿè®¡
        global_stats['total_errors'] += stats['invalid_answers']
        global_stats['total_warnings'] += stats['warnings']
        
        if not answers:
            global_stats['skipped_questions'] += 1
            continue
        
        global_stats['matched_questions'] += 1
        
        # æ„å»ºä¸Šä¸‹æ–‡
        ctx = "\n".join(f"å›ç­”{i+1}ï¼š{ans}" for i, ans in enumerate(answers))
        
        # æ£€æŸ¥ä¸Šä¸‹æ–‡æ ¼å¼
        ctx_valid, ctx_issues = ErrorChecker.check_context_format(ctx)
        if not ctx_valid:
            global_stats['context_format_errors'] += 1
            error_log.append({
                'type': 'context_error',
                'question': question[:50] + '...' if len(question) > 50 else question,
                'model': ",".join(found_models),
                'issues': ctx_issues,
                'context_preview': ctx[:100] + '...' if len(ctx) > 100 else ctx
            })
            # å¦‚æœä¸Šä¸‹æ–‡æ ¼å¼æœ‰ä¸¥é‡é—®é¢˜ï¼Œè·³è¿‡è¿™ä¸ªé—®é¢˜
            if any("ä¸ºç©º" in issue for issue in ctx_issues):
                global_stats['skipped_questions'] += 1
                continue
        
        # ç”Ÿæˆ prompt
        try:
            prompt = prompt_template.format(q=question, ctx=ctx)
            prompts.append(prompt)
        except Exception as e:
            error_log.append({
                'type': 'prompt_generation_error',
                'question': question[:50] + '...' if len(question) > 50 else question,
                'model': ",".join(found_models),
                'issues': [f"Promptç”Ÿæˆå¤±è´¥: {str(e)}"],
                'context_preview': ctx[:100] + '...' if len(ctx) > 100 else ctx
            })
            continue
        
        # æ˜¾ç¤ºè¿›åº¦
        if global_stats['matched_questions'] % 10 == 0:
            print(f"  Â· å·²å¤„ç† {global_stats['matched_questions']} ä¸ªåŒ¹é…çš„é—®é¢˜")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡ï¼š")
    print(f"  Â· æ€»é—®é¢˜æ•°: {global_stats['total_questions']}")
    print(f"  Â· åŒ¹é…é—®é¢˜æ•°: {global_stats['matched_questions']}")
    print(f"  Â· è·³è¿‡é—®é¢˜æ•°: {global_stats['skipped_questions']}")
    print(f"  Â· é”™è¯¯ç­”æ¡ˆæ•°: {global_stats['total_errors']}")
    print(f"  Â· è­¦å‘Šæ•°: {global_stats['total_warnings']}")
    print(f"  Â· ä¸Šä¸‹æ–‡æ ¼å¼é”™è¯¯: {global_stats['context_format_errors']}")
    print(f"  Â· ç”Ÿæˆpromptæ•°: {len(prompts)}")
    
    return prompts, error_log

def save_error_log(error_log: List[Dict], filepath: Path):
    """ä¿å­˜é”™è¯¯æ—¥å¿—"""
    if not error_log:
        print("  Â· æ²¡æœ‰é”™è¯¯æˆ–è­¦å‘Šéœ€è¦è®°å½•")
        return
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write("=== ç­”æ¡ˆçº é”™æ—¥å¿— ===\n\n")
        
        # åˆ†åˆ«ç»Ÿè®¡é”™è¯¯ã€è­¦å‘Šå’Œä¸Šä¸‹æ–‡é”™è¯¯
        errors = [e for e in error_log if e['type'] == 'error']
        warnings = [e for e in error_log if e['type'] == 'warning']
        context_errors = [e for e in error_log if e['type'] == 'context_error']
        prompt_errors = [e for e in error_log if e['type'] == 'prompt_generation_error']
        
        # å†™å…¥é”™è¯¯
        if errors:
            f.write(f"### é”™è¯¯ ({len(errors)} é¡¹) ###\n\n")
            for i, error in enumerate(errors, 1):
                f.write(f"{i}. é—®é¢˜: {error['question']}\n")
                f.write(f"   æ¨¡å‹: {error['model']}\n")
                f.write(f"   é”™è¯¯: {', '.join(error['issues'])}\n")
                f.write(f"   åŸå§‹ç­”æ¡ˆ: {error.get('raw_answer', 'N/A')}\n")
                f.write("-" * 50 + "\n")
        
        # å†™å…¥ä¸Šä¸‹æ–‡æ ¼å¼é”™è¯¯
        if context_errors:
            f.write(f"\n### ä¸Šä¸‹æ–‡æ ¼å¼é”™è¯¯ ({len(context_errors)} é¡¹) ###\n\n")
            for i, error in enumerate(context_errors, 1):
                f.write(f"{i}. é—®é¢˜: {error['question']}\n")
                f.write(f"   æ¨¡å‹: {error['model']}\n")
                f.write(f"   é—®é¢˜: {', '.join(error['issues'])}\n")
                f.write(f"   ä¸Šä¸‹æ–‡é¢„è§ˆ: {error.get('context_preview', 'N/A')}\n")
                f.write("-" * 50 + "\n")
        
        # å†™å…¥Promptç”Ÿæˆé”™è¯¯
        if prompt_errors:
            f.write(f"\n### Promptç”Ÿæˆé”™è¯¯ ({len(prompt_errors)} é¡¹) ###\n\n")
            for i, error in enumerate(prompt_errors, 1):
                f.write(f"{i}. é—®é¢˜: {error['question']}\n")
                f.write(f"   æ¨¡å‹: {error['model']}\n")
                f.write(f"   é”™è¯¯: {', '.join(error['issues'])}\n")
                f.write("-" * 50 + "\n")
        
        # å†™å…¥è­¦å‘Š
        if warnings:
            f.write(f"\n### è­¦å‘Š ({len(warnings)} é¡¹) ###\n\n")
            for i, warning in enumerate(warnings, 1):
                f.write(f"{i}. é—®é¢˜: {warning['question']}\n")
                f.write(f"   æ¨¡å‹: {warning['model']}\n")
                f.write(f"   è­¦å‘Š: {', '.join(warning['issues'])}\n")
                f.write(f"   ç­”æ¡ˆé¢„è§ˆ: {warning.get('answer_preview', 'N/A')}\n")
                f.write("-" * 50 + "\n")
    
    print(f"  Â· é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ°: {filepath}")
    print(f"    - ç­”æ¡ˆé”™è¯¯: {len(errors)} é¡¹")
    print(f"    - ä¸Šä¸‹æ–‡æ ¼å¼é”™è¯¯: {len(context_errors)} é¡¹")
    print(f"    - Promptç”Ÿæˆé”™è¯¯: {len(prompt_errors)} é¡¹")
    print(f"    - è­¦å‘Š: {len(warnings)} é¡¹")

# === 5. ä¸»ç¨‹åº ===
def main():
    print("ğŸ“– å¼€å§‹å¤„ç†æ•°æ®...")
    print(f"  Â· å·¥ä½œç›®å½•ï¼š{BASE_DIR}")
    print(f"  Â· æ¨¡ç³ŠåŒ¹é…æ¨¡å‹ï¼šgemini, grok, doubao")
    print(f"  Â· è¾“å‡ºæ ¼å¼ï¼šTXTæ–‡ä»¶ï¼ˆæ¯ä¸ªpromptä¸ºä¸€æ®µï¼‰")
    
    # è¯»å– prompt æ¨¡æ¿
    prompt_template = load_prompt_template()
    
    # è¯»å– JSON æ•°æ®
    print(f"\nğŸ“– è¯»å– JSON æ–‡ä»¶ï¼š{json_path}")
    try:
        with open(json_path, encoding="utf-8") as f:
            data = json.load(f)
        
        # æ£€æŸ¥æ•°æ®ç»“æ„
        if "questions" in data:
            questions_data = data["questions"]
            print(f"  Â· æ‰¾åˆ° {len(questions_data)} ä¸ªé—®é¢˜")
        else:
            print("âŒ é”™è¯¯ï¼šJSONæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ° 'questions' å­—æ®µ")
            return
            
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {json_path}")
        return
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯ï¼šJSONè§£æå¤±è´¥ï¼š{e}")
        return
    
    # === ç”Ÿæˆpromptså¹¶å†™å…¥TXTæ–‡ä»¶ ===
    print("\nâš™ï¸ ç”Ÿæˆ prompt åˆ—è¡¨...")
    all_prompts, error_log = build_prompts(questions_data, prompt_template)
    
    if all_prompts:
        # å†™å…¥TXTæ–‡ä»¶
        print(f"\nğŸ“ å†™å…¥ TXT æ–‡ä»¶ï¼š{OUT_TXT}")
        with open(OUT_TXT, "w", encoding="utf-8") as f:
            for i, prompt in enumerate(all_prompts):
                f.write(prompt)
                # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªpromptï¼Œæ·»åŠ åˆ†éš”çº¿
                if i < len(all_prompts) - 1:
                    f.write("\n-------------------\n")
        
        print(f"âœ… æˆåŠŸå†™å…¥ {len(all_prompts)} ä¸ªprompt")
        
        # ä¿å­˜é”™è¯¯æ—¥å¿—
        print(f"\nğŸ“ ä¿å­˜é”™è¯¯æ—¥å¿—...")
        save_error_log(error_log, ERROR_LOG)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_chars = sum(len(prompt) for prompt in all_prompts)
        avg_chars = total_chars // len(all_prompts) if all_prompts else 0
        print(f"\nğŸ“Š å†…å®¹ç»Ÿè®¡ï¼š")
        print(f"  Â· æ€»promptæ•°é‡: {len(all_prompts)}")
        print(f"  Â· æ€»å­—ç¬¦æ•°: {total_chars:,}")
        print(f"  Â· å¹³å‡æ¯ä¸ªprompt: {avg_chars} å­—ç¬¦")
    else:
        print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰ç”Ÿæˆä»»ä½•prompt")
    
    print("\nğŸ‰ å®Œæˆï¼")

if __name__ == "__main__":
    main()