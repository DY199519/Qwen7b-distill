#!/usr/bin/env python
# coding: utf-8
"""
single_question_grade_combination.py
------------------------------------
è¯»å–å¤š-combination ç­”æ¡ˆ JSONï¼Œé’ˆå¯¹æŒ‡å®šç»„åˆè‡ªåŠ¨æ‰“åˆ†å¹¶æŒç»­ä¿å­˜è¿›åº¦ã€‚
å¢å¼ºåŠŸèƒ½ï¼š
- æ•°æ®è´¨é‡æ£€æŸ¥
- è‡ªåŠ¨é‡æ–°è¯„åˆ†å¼‚å¸¸ç»“æœ
- è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
- è¯„åˆ†ä¸€è‡´æ€§éªŒè¯
"""

import json, re, os, time
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import httpx
from openai import OpenAI
from tqdm import tqdm
from datetime import datetime
import statistics

# ========== é…ç½®é€‰é¡¹ =========================================================
# æ”¯æŒå¤šç§å­—æ®µåçš„è¯„åˆ†
FIELDS_TO_GRADE = ["3+1_reply", "default_reply"]  # æŒ‰ä¼˜å…ˆçº§æ’åºçš„å­—æ®µååˆ—è¡¨
SAVE_INTERVAL = 1  # æ¯ N é¢˜ä¿å­˜ä¸€æ¬¡

# è¯„åˆ†è´¨é‡é˜ˆå€¼
MIN_VALID_TRIALS = 2  # æœ€å°‘éœ€è¦æˆåŠŸçš„è¯„åˆ†æ¬¡æ•°
MAX_SCORE_VARIANCE = 5  # å¤šæ¬¡è¯„åˆ†çš„æœ€å¤§æ–¹å·®ï¼ˆç”¨äºæ£€æµ‹ä¸ä¸€è‡´ï¼‰
SUSPICIOUS_SCORE_THRESHOLD = 10  # ä½äºæ­¤åˆ†æ•°è§†ä¸ºå¯ç–‘ï¼Œéœ€è¦é‡æ–°è¯„åˆ†

# ========== OpenAI åˆå§‹åŒ– ====================================================
httpx_client = httpx.Client(verify=False)
os.environ["OPENAI_API_KEY"]  = "sk-TlCq2TfX7oLuXzZMD1A3681285A2460bA26b6f0cEa5517Aa"
os.environ["OPENAI_BASE_URL"] = "https://vir.vimsai.com/v1"
client = OpenAI(http_client=httpx_client)

# ========== è·¯å¾„è®¾ç½® =========================================================
INPUT_PATH = r"D:\project7\MM\result\3+1\deepseek_answers_without_summary3+1-9400-10000.json"
OUTPUT_DIR = r"D:\project7\MM\result\3+1"
Path(OUTPUT_DIR).mkdir(exist_ok=True, parents=True)
OUTPUT_FILE = Path(OUTPUT_DIR) / "grades-3+1-9400-10000.json"

# æ—¥å¿—æ–‡ä»¶
LOG_FILE = Path(OUTPUT_DIR) / f"grading_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

# ========== æ—¥å¿—è®°å½•å™¨ =======================================================
class Logger:
    """ç®€å•çš„æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_file: Path):
        self.log_file = log_file
        self.start_time = datetime.now()
        self._write(f"=== è¯„åˆ†å¼€å§‹: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')} ===\n")
    
    def _write(self, message: str):
        """å†™å…¥æ—¥å¿—"""
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(f"[{datetime.now().strftime('%H:%M:%S')}] {message}\n")
    
    def info(self, message: str):
        """è®°å½•ä¿¡æ¯"""
        self._write(f"INFO: {message}")
        print(f"ğŸ“ {message}")
    
    def error(self, message: str):
        """è®°å½•é”™è¯¯"""
        self._write(f"ERROR: {message}")
        print(f"âŒ {message}")
    
    def warning(self, message: str):
        """è®°å½•è­¦å‘Š"""
        self._write(f"WARNING: {message}")
        print(f"âš ï¸ {message}")
    
    def success(self, message: str):
        """è®°å½•æˆåŠŸ"""
        self._write(f"SUCCESS: {message}")
        print(f"âœ… {message}")

# ========== è¯„åˆ†è´¨é‡æ£€æŸ¥å™¨ ===================================================
class ScoreValidator:
    """è¯„åˆ†è´¨é‡éªŒè¯å™¨"""
    
    @staticmethod
    def validate_single_score(scores: Dict[str, int]) -> Tuple[bool, List[str]]:
        """éªŒè¯å•æ¬¡è¯„åˆ†çš„æœ‰æ•ˆæ€§"""
        issues = []
        
        # æ£€æŸ¥åˆ†æ•°èŒƒå›´
        if not (0 <= scores["total"] <= 50):
            issues.append(f"æ€»åˆ†å¼‚å¸¸: {scores['total']}")
        
        for key in ["logic", "depth", "innovation", "accuracy", "completeness"]:
            if key not in scores:
                issues.append(f"ç¼ºå°‘{key}åˆ†æ•°")
            elif not (0 <= scores[key] <= 10):
                issues.append(f"{key}åˆ†æ•°å¼‚å¸¸: {scores[key]}")
        
        # æ£€æŸ¥æ€»åˆ†æ˜¯å¦ç­‰äºå„é¡¹ä¹‹å’Œ
        expected_total = sum(scores.get(k, 0) for k in ["logic", "depth", "innovation", "accuracy", "completeness"])
        if scores["total"] != expected_total:
            issues.append(f"æ€»åˆ†({scores['total']})ä¸å„é¡¹ä¹‹å’Œ({expected_total})ä¸ç¬¦")
        
        return len(issues) == 0, issues
    
    @staticmethod
    def validate_multiple_scores(all_scores: List[Dict[str, int]]) -> Tuple[bool, List[str]]:
        """éªŒè¯å¤šæ¬¡è¯„åˆ†çš„ä¸€è‡´æ€§"""
        issues = []
        
        if len(all_scores) < MIN_VALID_TRIALS:
            issues.append(f"æœ‰æ•ˆè¯„åˆ†æ¬¡æ•°ä¸è¶³: {len(all_scores)} < {MIN_VALID_TRIALS}")
            return False, issues
        
        # è®¡ç®—æ€»åˆ†çš„æ–¹å·®
        totals = [s["total"] for s in all_scores]
        if len(totals) > 1:
            variance = statistics.variance(totals)
            if variance > MAX_SCORE_VARIANCE:
                issues.append(f"è¯„åˆ†ä¸€è‡´æ€§å·®ï¼Œæ–¹å·®: {variance:.2f} > {MAX_SCORE_VARIANCE}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸ä½åˆ†
        avg_total = sum(totals) / len(totals)
        if avg_total < SUSPICIOUS_SCORE_THRESHOLD:
            issues.append(f"å¹³å‡åˆ†è¿‡ä½: {avg_total:.2f} < {SUSPICIOUS_SCORE_THRESHOLD}")
        
        return len(issues) == 0, issues
    
    @staticmethod
    def validate_grading_result(result: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """éªŒè¯å®Œæ•´çš„è¯„åˆ†ç»“æœ"""
        issues = []
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ["question", "avg_scores", "avg_score_100", "num_valid_trials", "all_scores"]
        for field in required_fields:
            if field not in result:
                issues.append(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
        
        # æ£€æŸ¥è¯„åˆ†æ¬¡æ•°
        if result.get("num_valid_trials", 0) < MIN_VALID_TRIALS:
            issues.append(f"æœ‰æ•ˆè¯„åˆ†æ¬¡æ•°ä¸è¶³")
        
        # æ£€æŸ¥å¹³å‡åˆ†è®¡ç®—
        if "avg_score_100" in result and "avg_scores" in result:
            expected_100 = result["avg_scores"]["total"] * 2
            if abs(result["avg_score_100"] - expected_100) > 0.1:
                issues.append(f"ç™¾åˆ†åˆ¶åˆ†æ•°è®¡ç®—é”™è¯¯")
        
        return len(issues) == 0, issues

# ========== Prompt æ¨¡æ¿ =====================================================
PROMPT_TMPL = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šç­”é¢˜è¯„å®¡å‘˜ï¼Œè¯·å¯¹ä»¥ä¸‹ç­”æ¡ˆè¿›è¡Œè¯„åˆ†ï¼ŒæŒ‰ç…§ä»¥ä¸‹ 5 ä¸ªç»´åº¦æ‰“åˆ†ï¼š
1. é€»è¾‘æ€§   2. æ·±åº¦   3. åˆ›æ–°æ€§   4. å‡†ç¡®æ€§   5. å®Œæ•´æ€§
æ¯ç»´åº¦æ»¡åˆ† 5ï¼Œæ€»åˆ† 25ã€‚

è¯„åˆ†æ ¼å¼ç¤ºä¾‹ï¼ˆä¸¥æ ¼ç…§æŠ„æ•°å­—å’Œç©ºæ ¼ï¼‰ï¼š
15 3 3 3 3 3
ï¼ˆæ­¤è¡Œåé¢ç´§è·Ÿè¯„åˆ†ç†ç”±æ®µè½ï¼‰

### é—®é¢˜
{question}

### å›ç­”
{answer}

### è¾“å‡ºè¦æ±‚
- ç¬¬ä¸€è¡Œ **åªå†™ 6 ä¸ªæ•°å­—**ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼Œé¡ºåºæ˜¯ï¼šæ€»åˆ† é€»è¾‘ æ·±åº¦ åˆ›æ–° å‡†ç¡® å®Œæ•´
- ä¸è¦å†™ä»»ä½•æ–‡å­—ã€å•ä½æˆ–æ ‡ç‚¹
- ç¬¬äºŒè¡Œå¼€å§‹å†™è¯¦ç»†è¯„åˆ†ç†ç”±ï¼ˆè‡³å°‘ 2 æ®µï¼‰

1. é€»è¾‘æ€§ â€”â€” è®ºè¯ç»“æ„ã€å› æœé“¾æ¡æ˜¯å¦ä¸¥è°¨ï¼›  
2. æ·±åº¦   â€”â€” æ˜¯å¦å¼•ç”¨å­¦æœ¯æ¦‚å¿µ / æ•°æ® / å¤šè§’åº¦åˆ†æï¼›  
3. åˆ›æ–°æ€§ â€”â€” æ˜¯å¦æå‡ºæ–°è§‚ç‚¹æˆ–éé™ˆè¯æ»¥è°ƒçš„æ´è§ï¼›  
4. å‡†ç¡®æ€§ â€”â€” äº‹å®ã€æ•°æ®ã€æ¦‚å¿µæ˜¯å¦æ­£ç¡®ï¼›  
5. å®Œæ•´æ€§ â€”â€” æ˜¯å¦å……åˆ†å›ç­”é¢˜å¹²æ‰€æœ‰è¦ç‚¹ã€‚

**æ‰“åˆ†ç¡¬æ€§è§„åˆ™**ï¼ˆä¸€å®šè¦æ‰§è¡Œ,è¯·è°¨æ…æ‰“é«˜åˆ†ï¼‰ï¼š  
| å•ç»´å¾—åˆ† | è¯„ä»·åŸºå‡†ï¼ˆç¤ºä¾‹ï¼‰ |  
|----------|-----------------|  
| 5 | å‡ ä¹æ— ç¼ºé™·ï¼Œä»…å¯æŒ‘ç»†èŠ‚ |  
| 4  | æœ‰ 1â€“2 å¤„è½»å¾®ç¼ºé™· |  
| 3  | å‡ºç° **æ˜æ˜¾ç¼ºé™·** æˆ–é—æ¼è¦ç‚¹ |  
| 2  | å¤šå¤„ç¼ºé™·ï¼Œè®ºè¯/äº‹å®é”™è¯¯ >2 å¤„ |  
| 0â€“1  | å…³é”®é€»è¾‘ä¸æˆç«‹ï¼Œæˆ–äº‹å®é”™è¯¯ä¸¥é‡ |
ä¸¥æ ¼éµå®ˆæ ¼å¼ï¼Œç°åœ¨å¼€å§‹ï¼š
"""

# ---------------------------------------------------------------------------
def read_json_file(file_path: str) -> List[Dict[str, Any]]:
    """è¯»å– JSON æ–‡ä»¶"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    return []

# ---------------------------------------------------------------------------
def load_existing_results(output_file: Path, logger: Logger) -> Tuple[Dict[str, Any] | None, set, List[str]]:
    """
    åŠ è½½å·²æœ‰è¯„åˆ†è¿›åº¦å¹¶æ£€æŸ¥è´¨é‡
    è¿”å›: (å®Œæ•´æ•°æ®, æœ‰æ•ˆå®Œæˆé›†åˆ, éœ€è¦é‡æ–°è¯„åˆ†çš„é—®é¢˜åˆ—è¡¨)
    """
    if output_file.exists():
        try:
            with open(output_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            detailed_results = data.get("detailed_results", [])
            valid_done = set()
            need_regrade = []
            
            logger.info(f"æ£€æŸ¥å·²æœ‰è¯„åˆ†è´¨é‡...")
            
            for result in detailed_results:
                question = result["question"]
                is_valid, issues = ScoreValidator.validate_grading_result(result)
                
                if is_valid:
                    # å†æ£€æŸ¥å¤šæ¬¡è¯„åˆ†çš„ä¸€è‡´æ€§
                    if "all_scores" in result:
                        consistency_valid, consistency_issues = ScoreValidator.validate_multiple_scores(result["all_scores"])
                        if not consistency_valid:
                            is_valid = False
                            issues.extend(consistency_issues)
                
                if is_valid:
                    valid_done.add(question)
                else:
                    need_regrade.append(question)
                    logger.warning(f"é—®é¢˜ '{question[:40]}...' éœ€è¦é‡æ–°è¯„åˆ†: {', '.join(issues)}")
            
            logger.info(f"è¯„åˆ†è´¨é‡æ£€æŸ¥å®Œæˆï¼š")
            logger.info(f"  Â· æœ‰æ•ˆè¯„åˆ†: {len(valid_done)}")
            logger.info(f"  Â· éœ€è¦é‡è¯„: {len(need_regrade)}")
            
            return data, valid_done, need_regrade
            
        except Exception as e:
            logger.error(f"è¯»å–è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
    
    return None, set(), []

# ---------------------------------------------------------------------------
def save_progress(data: Dict[str, Any], output_file: Path, logger: Logger):
    """ä¿å­˜è¿›åº¦ï¼ŒåŒ…å«å¤‡ä»½æœºåˆ¶"""
    try:
        # åˆ›å»ºå¤‡ä»½
        if output_file.exists():
            backup_file = output_file.with_suffix('.backup.json')
            output_file.rename(backup_file)
        
        # ä¿å­˜æ–°æ–‡ä»¶
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.success(f"è¿›åº¦å·²ä¿å­˜è‡³ {output_file}")
        
        # åˆ é™¤å¤‡ä»½
        if output_file.with_suffix('.backup.json').exists():
            output_file.with_suffix('.backup.json').unlink()
            
    except Exception as e:
        logger.error(f"ä¿å­˜å¤±è´¥: {e}")
        # æ¢å¤å¤‡ä»½
        backup_file = output_file.with_suffix('.backup.json')
        if backup_file.exists():
            backup_file.rename(output_file)
            logger.info("å·²ä»å¤‡ä»½æ¢å¤")

# ---------------------------------------------------------------------------
def parse_response(raw: str, logger: Logger) -> Tuple[Dict[str, int], str]:
    """è§£æ GPT è¾“å‡ºï¼Œå¢åŠ é”™è¯¯å¤„ç†"""
    keys = ["total", "logic", "depth", "innovation", "accuracy", "completeness"]
    lines = [l.strip() for l in raw.splitlines() if l.strip()]

    # æ‰¾åˆ†æ•°å­—ä¸²
    score_line = None
    for line in lines:
        # å°è¯•æ‰¾åˆ°åŒ…å«6ä¸ªæ•°å­—çš„è¡Œ
        numbers = re.findall(r'\b\d+\b', line)
        if len(numbers) >= 6:
            score_line = line
            break
    
    if not score_line:
        raise ValueError("æ‰¾ä¸åˆ°å®Œæ•´åˆ†æ•°è¡Œ")
    
    nums = list(map(int, re.findall(r'\b\d+\b', score_line)[:6]))
    
    # éªŒè¯åˆ†æ•°
    scores = dict(zip(keys, nums))
    is_valid, issues = ScoreValidator.validate_single_score(scores)
    if not is_valid:
        logger.warning(f"åˆ†æ•°éªŒè¯å¤±è´¥: {', '.join(issues)}")
        raise ValueError(f"åˆ†æ•°éªŒè¯å¤±è´¥: {', '.join(issues)}")

    # æå–è¯„è®º
    score_line_idx = lines.index(score_line)
    commentary = "\n".join(lines[score_line_idx + 1:]).strip()
    if not commentary:
        raise ValueError("ç¼ºå°‘è¯„åˆ†ç†ç”±")

    return scores, commentary

# ---------------------------------------------------------------------------
def ask_and_parse(prompt: str,
                  logger: Logger,
                  model: str = "gpt-4o",
                  max_attempts: int = 6,
                  backoff_base: int = 2):
    """è°ƒç”¨APIå¹¶è§£æç»“æœï¼Œå¢åŠ é”™è¯¯å¤„ç†"""
    for attempt in range(1, max_attempts + 1):
        try:
            resp = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0
            )
            raw = resp.choices[0].message.content.strip()
            scores, detail = parse_response(raw, logger)
            return scores, detail, raw
        except Exception as e:
            wait = backoff_base ** attempt
            logger.warning(f"ç¬¬ {attempt}/{max_attempts} æ¬¡å¤±è´¥: {e}ï¼Œ{wait}s åé‡è¯•")
            time.sleep(wait)
    return None

# ---------------------------------------------------------------------------
def grade_single(question: str, answer: str, logger: Logger, trials: int = 3, max_retries: int = 2):
    """
    å¯¹å•ä¸ªé—®é¢˜è¿›è¡Œè¯„åˆ†ï¼Œå¢åŠ é‡è¯•æœºåˆ¶
    max_retries: å¦‚æœæ‰€æœ‰trialséƒ½å¤±è´¥ï¼Œæœ€å¤šé‡è¯•çš„è½®æ•°
    """
    prompt = PROMPT_TMPL.format(question=question, answer=answer)
    
    for retry in range(max_retries + 1):
        if retry > 0:
            logger.info(f"ç¬¬ {retry + 1} è½®é‡è¯•...")
            time.sleep(5 * retry)  # é€’å¢ç­‰å¾…
        
        all_scores, all_cmts, raws = [], [], []
        
        for t in range(trials):
            res = ask_and_parse(prompt, logger)
            if not res:
                logger.warning(f"  ç¬¬ {t+1} æ¬¡è¯„åˆ†å¤±è´¥")
                continue
            
            score, cmt, raw = res
            all_scores.append(score)
            all_cmts.append(cmt)
            raws.append(raw)
            logger.info(f"  ç¬¬ {t+1} æ¬¡å¾—åˆ†ï¼š{score['total']}/50")
        
        # æ£€æŸ¥è¯„åˆ†ä¸€è‡´æ€§
        if len(all_scores) >= MIN_VALID_TRIALS:
            is_valid, issues = ScoreValidator.validate_multiple_scores(all_scores)
            if is_valid:
                # è®¡ç®—å¹³å‡åˆ†
                avg = {k: round(sum(s[k] for s in all_scores) / len(all_scores), 2)
                       for k in all_scores[0]}
                avg100 = round(avg["total"] * 2, 2)
                
                return {
                    "question": question,
                    "avg_scores": avg,
                    "avg_score_100": avg100,
                    "num_valid_trials": len(all_scores),
                    "all_scores": all_scores,
                    "all_commentaries": all_cmts,
                    "all_gpt_raws": raws,
                    "retry_count": retry
                }
            else:
                logger.warning(f"è¯„åˆ†ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {', '.join(issues)}")
    
    logger.error(f"ç»è¿‡ {max_retries + 1} è½®å°è¯•ä»æ— æ³•è·å¾—æœ‰æ•ˆè¯„åˆ†")
    return None

# ---------------------------------------------------------------------------
def get_field_value(record: Dict[str, Any], fields: List[str]) -> Tuple[Optional[str], Optional[str]]:
    """
    ä»è®°å½•ä¸­è·å–å­—æ®µå€¼ï¼ŒæŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„å­—æ®µå
    è¿”å›: (å­—æ®µå€¼, ä½¿ç”¨çš„å­—æ®µå)
    """
    for field in fields:
        if field in record and record[field] and record[field].strip():
            return record[field], field
    return None, None

# ---------------------------------------------------------------------------
def grade_replies(records: List[Dict[str, Any]], logger: Logger):
    """è¯„åˆ†replyå­—æ®µï¼Œæ”¯æŒå¤šç§å­—æ®µå"""
    logger.info(f"===== è¯„åˆ† {'/'.join(FIELDS_TO_GRADE)} å­—æ®µ =====")

    # è¾“å‡ºæ–‡ä»¶å
    prev, valid_done_set, need_regrade = load_existing_results(OUTPUT_FILE, logger)
    
    # è·å–å·²æœ‰çš„æœ‰æ•ˆç»“æœ
    existing_results = []
    if prev:
        for result in prev.get("detailed_results", []):
            if result["question"] not in need_regrade:
                existing_results.append(result)
    
    # ç­›é€‰æœ‰æ•ˆæ•°æ®å¹¶è®°å½•ä½¿ç”¨çš„å­—æ®µ
    items = []
    field_usage = {}  # è®°å½•æ¯ä¸ªé—®é¢˜ä½¿ç”¨çš„å­—æ®µå
    
    for record in records:
        value, field_used = get_field_value(record, FIELDS_TO_GRADE)
        if value:
            items.append(record)
            field_usage[record["question"]] = field_used
    
    if not items:
        logger.error(f"æœªæ‰¾åˆ°åŒ…å« {'/'.join(FIELDS_TO_GRADE)} å­—æ®µçš„æœ‰æ•ˆæ•°æ®")
        return
    
    # ç»Ÿè®¡å­—æ®µä½¿ç”¨æƒ…å†µ
    field_counts = {}
    for field in field_usage.values():
        field_counts[field] = field_counts.get(field, 0) + 1
    
    logger.info("å­—æ®µä½¿ç”¨ç»Ÿè®¡ï¼š")
    for field, count in field_counts.items():
        logger.info(f"  Â· {field}: {count} é¢˜")

    # è®¡ç®—å¾…å¤„ç†é¡¹
    # 1. å…¨æ–°çš„é—®é¢˜
    new_questions = [d for d in items if d["question"] not in valid_done_set and d["question"] not in need_regrade]
    # 2. éœ€è¦é‡æ–°è¯„åˆ†çš„é—®é¢˜
    regrade_questions = [d for d in items if d["question"] in need_regrade]
    
    pending = new_questions + regrade_questions
    
    logger.info(f"æ•°æ®ç»Ÿè®¡ï¼š")
    logger.info(f"  Â· æ€»é¢˜æ•°: {len(items)}")
    logger.info(f"  Â· å·²æœ‰æ•ˆå®Œæˆ: {len(valid_done_set)}")
    logger.info(f"  Â· éœ€è¦é‡è¯„: {len(regrade_questions)}")
    logger.info(f"  Â· å…¨æ–°é¢˜ç›®: {len(new_questions)}")
    logger.info(f"  Â· å¾…å¤„ç†æ€»æ•°: {len(pending)}")

    # ä¸»å¾ªç¯
    results = existing_results.copy()
    all_totals = [r["avg_scores"]["total"] for r in results]
    all_totals100 = [r["avg_score_100"] for r in results]
    
    # æŒ‰å­—æ®µåˆ†ç±»çš„ç»Ÿè®¡
    field_stats = {field: {"count": 0, "total_score": 0} for field in FIELDS_TO_GRADE}
    
    failed_questions = []
    regraded_count = 0

    for idx, item in enumerate(pending, 1):
        q = item["question"]
        field_used = field_usage[q]
        a = item[field_used]
        
        is_regrade = q in need_regrade
        
        if is_regrade:
            logger.info(f"\nğŸ”„ [{idx}/{len(pending)}] é‡æ–°è¯„åˆ† ({field_used}): {q[:40]}...")
            regraded_count += 1
        else:
            logger.info(f"\n[{idx}/{len(pending)}] è¯„åˆ† ({field_used}): {q[:40]}...")
        
        res = grade_single(q, a, logger)
        
        if res:
            res["field_graded"] = field_used  # è®°å½•å®é™…ä½¿ç”¨çš„å­—æ®µ
            res["is_regraded"] = is_regrade
            res["grading_timestamp"] = datetime.now().isoformat()
            
            results.append(res)
            all_totals.append(res["avg_scores"]["total"])
            all_totals100.append(res["avg_score_100"])
            
            # æ›´æ–°å­—æ®µç»Ÿè®¡
            field_stats[field_used]["count"] += 1
            field_stats[field_used]["total_score"] += res["avg_scores"]["total"]
            
            # å†æ¬¡éªŒè¯ç»“æœ
            is_valid, issues = ScoreValidator.validate_grading_result(res)
            if not is_valid:
                logger.warning(f"è¯„åˆ†ç»“æœéªŒè¯å¤±è´¥: {', '.join(issues)}")
        else:
            failed_questions.append({
                "question": q,
                "field_used": field_used,
                "reason": "æ— æ³•è·å¾—æœ‰æ•ˆè¯„åˆ†",
                "timestamp": datetime.now().isoformat()
            })

        # å®šæœŸä¿å­˜
        if idx % SAVE_INTERVAL == 0:
            # è®¡ç®—å„å­—æ®µçš„å¹³å‡åˆ†
            field_averages = {}
            for field, stats in field_stats.items():
                if stats["count"] > 0:
                    field_averages[field] = {
                        "count": stats["count"],
                        "average": round(stats["total_score"] / stats["count"], 2)
                    }
            
            stats = {
                "fields_graded": FIELDS_TO_GRADE,
                "total_questions": len(items),
                "valid_grades": len(all_totals),
                "failed_grades": len(failed_questions),
                "regraded_count": regraded_count,
                "total_average": round(sum(all_totals)/len(all_totals), 2) if all_totals else 0,
                "total_average_100": round(sum(all_totals100)/len(all_totals100), 2) if all_totals100 else 0,
                "field_statistics": field_averages,
                "last_update": datetime.now().isoformat()
            }
            save_progress({"statistics": stats, "detailed_results": results}, OUTPUT_FILE, logger)

    # æœ€ç»ˆç»Ÿè®¡å’Œä¿å­˜
    if all_totals:
        # è®¡ç®—å„å­—æ®µçš„æœ€ç»ˆå¹³å‡åˆ†
        field_averages = {}
        for field in FIELDS_TO_GRADE:
            field_results = [r for r in results if r.get("field_graded") == field]
            if field_results:
                field_scores = [r["avg_scores"]["total"] for r in field_results]
                field_averages[field] = {
                    "count": len(field_results),
                    "average": round(sum(field_scores) / len(field_scores), 2),
                    "average_100": round(sum(field_scores) / len(field_scores) * 2, 2)
                }
        
        stats = {
            "fields_graded": FIELDS_TO_GRADE,
            "total_questions": len(items),
            "valid_grades": len(all_totals),
            "failed_grades": len(failed_questions),
            "regraded_count": regraded_count,
            "total_average": round(sum(all_totals)/len(all_totals), 2),
            "total_average_100": round(sum(all_totals100)/len(all_totals100), 2),
            "field_statistics": field_averages,
            "score_distribution": {
                "0-20": len([s for s in all_totals if s < 20]),
                "20-30": len([s for s in all_totals if 20 <= s < 30]),
                "30-40": len([s for s in all_totals if 30 <= s < 40]),
                "40-50": len([s for s in all_totals if 40 <= s <= 50])
            },
            "completion_time": datetime.now().isoformat()
        }
        
        final_data = {
            "statistics": stats,
            "detailed_results": results
        }
        
        # ä¿å­˜å¤±è´¥è®°å½•
        if failed_questions:
            final_data["failed_questions"] = failed_questions
            
            # å•ç‹¬ä¿å­˜å¤±è´¥è®°å½•æ–‡ä»¶
            failed_file = Path(OUTPUT_DIR) / f"failed_grades_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(failed_file, 'w', encoding='utf-8') as f:
                json.dump(failed_questions, f, ensure_ascii=False, indent=2)
            logger.warning(f"å¤±è´¥è®°å½•å·²ä¿å­˜åˆ°: {failed_file}")
        
        save_progress(final_data, OUTPUT_FILE, logger)
        
        logger.success(f"\nğŸ“Š è¯„åˆ†å®Œæˆï¼")
        logger.success(f"  Â· æ€»å¹³å‡åˆ†: {stats['total_average']}/50 ({stats['total_average_100']}åˆ†)")
        logger.success(f"  Â· æœ‰æ•ˆè¯„åˆ†: {stats['valid_grades']}")
        logger.success(f"  Â· å¤±è´¥: {stats['failed_grades']}")
        logger.success(f"  Â· é‡æ–°è¯„åˆ†: {stats['regraded_count']}")
        
        # æ˜¾ç¤ºå„å­—æ®µç»Ÿè®¡
        logger.success(f"\nğŸ“ˆ å„å­—æ®µç»Ÿè®¡ï¼š")
        for field, field_stat in field_averages.items():
            logger.success(f"  Â· {field}: {field_stat['count']} é¢˜, å¹³å‡ {field_stat['average']}/50 ({field_stat['average_100']}åˆ†)")

# ---------------------------------------------------------------------------
def main():
    logger = Logger(LOG_FILE)
    logger.info("å¼€å§‹è¯„åˆ†ä»»åŠ¡")
    
    data = read_json_file(INPUT_PATH)
    if not data:
        logger.error("è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥")
        return

    # å¯¹replyå­—æ®µè¿›è¡Œè¯„åˆ†ï¼ˆæ”¯æŒå¤šç§å­—æ®µåï¼‰
    grade_replies(data, logger)
    
    # è®¡ç®—æ€»è€—æ—¶
    elapsed = datetime.now() - logger.start_time
    logger.info(f"ä»»åŠ¡å®Œæˆï¼Œæ€»è€—æ—¶: {elapsed}")

# ---------------------------------------------------------------------------
if __name__ == "__main__":
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"ğŸ“„ è¾“å…¥æ–‡ä»¶: {INPUT_PATH}")
    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {LOG_FILE}")
    print("-" * 60)
    
    main()