#!/usr/bin/env python
# coding: utf-8
"""
grade_quality_separator.py
--------------------------
æ ¹æ®ç­”æ¡ˆè´¨é‡æ£€æŸ¥ç»“æœï¼Œå°†è¯„åˆ†æ–‡ä»¶åˆ†æˆä¸¤éƒ¨åˆ†ï¼š
1. æœ‰è´¨é‡é—®é¢˜é¢˜ç›®çš„è¯„åˆ†ï¼ˆä¸å¯é ï¼‰
2. è´¨é‡è‰¯å¥½é¢˜ç›®çš„è¯„åˆ†ï¼ˆå¯é ï¼‰
"""

import json
import re
from pathlib import Path
from typing import Dict, Any, List, Tuple, Set
from datetime import datetime

# ========= é…ç½®å‚æ•° ===========================================================
# è¾“å…¥è·¯å¾„
INPUT_DIR = Path(r"D:\project7\MM\result")

# ç­”æ¡ˆæ–‡ä»¶ï¼ˆç”¨äºæ£€æŸ¥è´¨é‡ï¼‰
ANSWERS_FILE = INPUT_DIR / "multi_model_answer-1-700.json"

# è¯„åˆ†æ–‡ä»¶ï¼ˆéœ€è¦æ ¹æ®è´¨é‡æ£€æŸ¥ç»“æœåˆ†ç¦»ï¼‰
GRADES_FILE = INPUT_DIR / "3+1" / "grades-3+1-700-1700.json"

# è¾“å‡ºç›®å½•
OUTPUT_DIR = INPUT_DIR / "quality_separated"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# è¾“å‡ºæ–‡ä»¶
PROBLEMATIC_GRADES_FILE = OUTPUT_DIR / "grades_with_quality_issues.json"
GOOD_GRADES_FILE = OUTPUT_DIR / "grades_without_issues.json"
QUALITY_CHECK_REPORT = OUTPUT_DIR / "quality_check_report.json"
PROBLEMATIC_ANSWERS_FILE = OUTPUT_DIR / "answers_with_quality_issues.json"  # æ–°å¢ï¼šæœ‰é—®é¢˜çš„ç­”æ¡ˆæ–‡ä»¶

# è´¨é‡æ£€æŸ¥å‚æ•°
MIN_ANSWER_LENGTH = 100  # æœ€å°ç­”æ¡ˆé•¿åº¦
MIN_COMPLETE_LENGTH = 50  # å®Œæ•´æ€§æœ€å°é•¿åº¦

# éœ€è¦æ£€æŸ¥çš„æ¨¡å‹
MODELS_TO_CHECK = [
    "gemini-2.5-flash",
    "grok-3", 
    "doubao-pro-256k",
    "deepseek-v3"
]

# é”™è¯¯æ¨¡å¼
ERROR_PATTERNS = [
    r'^error:',
    r'^exception:',
    r'^\s*$',
    r'^null$',
    r'^undefined$',
    r'^N/A$',
    r'request failed',
    r'rate limit',
    r'timeout',
    r'\[ERROR',
    r'APIè°ƒç”¨å¤±è´¥'
]

# ========= å·¥å…·å‡½æ•° ===========================================================
def load_json_file(file_path: Path) -> Dict[str, Any]:
    """åŠ è½½JSONæ–‡ä»¶"""
    if not file_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return {}
    
    try:
        with file_path.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return {}

def save_json_file(data: Any, file_path: Path, description: str = ""):
    """ä¿å­˜JSONæ–‡ä»¶"""
    try:
        with file_path.open("w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… {description}å·²ä¿å­˜åˆ°: {file_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

def check_answer_quality(answer_text: str) -> Tuple[bool, str]:
    """
    æ£€æŸ¥ç­”æ¡ˆè´¨é‡ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªè„šæœ¬çš„æ ‡å‡†ï¼‰
    è¿”å›: (æ˜¯å¦æœ‰é—®é¢˜, é—®é¢˜æè¿°)
    """
    # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
    if not answer_text or answer_text.strip() == "":
        return True, "ç©ºç­”æ¡ˆ"
    
    answer_text = answer_text.strip()
    
    # æ£€æŸ¥é•¿åº¦
    if len(answer_text) < MIN_COMPLETE_LENGTH:
        return True, f"ç­”æ¡ˆè¿‡çŸ­({len(answer_text)}å­—ç¬¦)"
    
    # ç®€å•æ£€æŸ¥ï¼šæ˜¯å¦ä»¥å¸¸è§çš„å®Œæ•´æ ‡ç‚¹ç»“å°¾
    if answer_text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
        return False, "å®Œæ•´"
    
    # å¦‚æœæ²¡æœ‰æ ‡ç‚¹ç»“å°¾ï¼Œæ£€æŸ¥é•¿åº¦
    if len(answer_text) < MIN_ANSWER_LENGTH:
        return True, f"æ— ç»“å°¾æ ‡ç‚¹ä¸”è¾ƒçŸ­({len(answer_text)}å­—ç¬¦)"
    
    # é•¿ç­”æ¡ˆä½†æ— æ ‡ç‚¹ï¼Œä¹Ÿè§†ä¸ºä¸å®Œæ•´ï¼ˆä¸ç¬¬ä¸€ä¸ªè„šæœ¬ä¸€è‡´ï¼‰
    return True, f"æ— ç»“å°¾æ ‡ç‚¹({len(answer_text)}å­—ç¬¦)"

def check_answers_file(file_path: Path) -> Tuple[Set[str], Dict[str, List[Dict]], Dict[str, Any]]:
    """
    æ£€æŸ¥ç­”æ¡ˆæ–‡ä»¶ï¼Œæ‰¾å‡ºæœ‰è´¨é‡é—®é¢˜çš„é¢˜ç›®
    è¿”å›: (æœ‰é—®é¢˜çš„é¢˜ç›®é›†åˆ, é—®é¢˜è¯¦æƒ…, åŸå§‹questionsæ•°æ®)
    """
    print(f"\nğŸ” å¼€å§‹æ£€æŸ¥ç­”æ¡ˆæ–‡ä»¶: {file_path}")
    
    # åŠ è½½æ•°æ®
    data = load_json_file(file_path)
    if not data:
        return set(), {}, {}
    
    # è·å–questionséƒ¨åˆ†
    questions_data = data.get("questions", {})
    if not questions_data:
        print("âŒ æœªæ‰¾åˆ°questionså­—æ®µ")
        return set(), {}, {}
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(questions_data)} ä¸ªé—®é¢˜")
    
    # æ£€æŸ¥æ¯ä¸ªé¢˜ç›®
    problematic_questions = set()
    problem_details = {}
    
    total_checks = 0
    total_issues = 0
    
    for question, question_data in questions_data.items():
        answers = question_data.get("answers", {})
        question_has_issue = False
        question_issues = []
        
        # æ£€æŸ¥æ¯ä¸ªæ¨¡å‹çš„ç­”æ¡ˆ
        for model in MODELS_TO_CHECK:
            total_checks += 1
            
            if model not in answers:
                question_has_issue = True
                question_issues.append({
                    "model": model,
                    "issue": "ç­”æ¡ˆç¼ºå¤±",
                    "details": "è¯¥æ¨¡å‹æ²¡æœ‰ç­”æ¡ˆ"
                })
                total_issues += 1
            else:
                answer_list = answers[model]
                if not answer_list or len(answer_list) == 0:
                    question_has_issue = True
                    question_issues.append({
                        "model": model,
                        "issue": "ç©ºç­”æ¡ˆåˆ—è¡¨",
                        "details": "ç­”æ¡ˆåˆ—è¡¨ä¸ºç©º"
                    })
                    total_issues += 1
                else:
                    answer_text = answer_list[0].get("answer", "")
                    has_problem, problem_desc = check_answer_quality(answer_text)
                    
                    if has_problem:
                        question_has_issue = True
                        question_issues.append({
                            "model": model,
                            "issue": problem_desc,
                            "answer_length": len(answer_text),
                            "answer_preview": answer_text[:100] + "..." if len(answer_text) > 100 else answer_text
                        })
                        total_issues += 1
        
        # è®°å½•æœ‰é—®é¢˜çš„é¢˜ç›®
        if question_has_issue:
            problematic_questions.add(question)
            problem_details[question] = question_issues
    
    print(f"\nğŸ“Š æ£€æŸ¥å®Œæˆ:")
    print(f"  Â· æ€»æ£€æŸ¥é¡¹: {total_checks}")
    print(f"  Â· å‘ç°é—®é¢˜: {total_issues}")
    print(f"  Â· æœ‰é—®é¢˜çš„é¢˜ç›®: {len(problematic_questions)}")
    print(f"  Â· æ²¡é—®é¢˜çš„é¢˜ç›®: {len(questions_data) - len(problematic_questions)}")
    
    return problematic_questions, problem_details, questions_data

def separate_grades_by_quality(grades_file: Path, 
                             problematic_questions: Set[str],
                             problem_details: Dict[str, List[Dict]],
                             questions_data: Dict[str, Any]) -> Tuple[int, int]:
    """
    æ ¹æ®è´¨é‡é—®é¢˜åˆ†ç¦»è¯„åˆ†æ–‡ä»¶ï¼Œå¹¶ä¿å­˜æœ‰é—®é¢˜çš„ç­”æ¡ˆ
    è¿”å›: (æœ‰é—®é¢˜çš„è¯„åˆ†æ•°, æ²¡é—®é¢˜çš„è¯„åˆ†æ•°)
    """
    print(f"\nğŸ“‚ å¼€å§‹å¤„ç†è¯„åˆ†æ–‡ä»¶: {grades_file}")
    
    # åŠ è½½è¯„åˆ†æ•°æ®
    grades_data = load_json_file(grades_file)
    if not grades_data:
        return 0, 0
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯å’Œè¯¦ç»†ç»“æœ
    statistics = grades_data.get("statistics", {})
    detailed_results = grades_data.get("detailed_results", [])
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(detailed_results)} ä¸ªè¯„åˆ†ç»“æœ")
    
    # åˆ†ç¦»æ•°æ®
    problematic_grades = []
    good_grades = []
    problematic_answers = {}  # æ–°å¢ï¼šæ”¶é›†æœ‰é—®é¢˜çš„ç­”æ¡ˆ
    
    for grade_item in detailed_results:
        question = grade_item.get("question", "")
        
        if question in problematic_questions:
            # æ·»åŠ è´¨é‡é—®é¢˜ä¿¡æ¯
            grade_item["quality_issues"] = problem_details.get(question, [])
            grade_item["has_quality_issues"] = True
            problematic_grades.append(grade_item)
            
            # æ”¶é›†æœ‰é—®é¢˜çš„ç­”æ¡ˆæ•°æ®
            if question in questions_data:
                problematic_answers[question] = questions_data[question]
        else:
            grade_item["has_quality_issues"] = False
            good_grades.append(grade_item)
    
    print(f"\nğŸ“Š åˆ†ç¦»ç»“æœ:")
    print(f"  Â· æœ‰è´¨é‡é—®é¢˜çš„è¯„åˆ†: {len(problematic_grades)}")
    print(f"  Â· è´¨é‡è‰¯å¥½çš„è¯„åˆ†: {len(good_grades)}")
    print(f"  Â· æœ‰è´¨é‡é—®é¢˜çš„ç­”æ¡ˆ: {len(problematic_answers)}")
    
    # ä¿å­˜æœ‰é—®é¢˜çš„ç­”æ¡ˆï¼ˆä½¿ç”¨åŸå§‹æ ¼å¼ï¼‰
    if problematic_answers:
        # è®¡ç®—æœ‰é—®é¢˜ç­”æ¡ˆçš„æ±‡æ€»ä¿¡æ¯
        all_models = set()
        total_answer_count = 0
        for q_data in problematic_answers.values():
            answers = q_data.get("answers", {})
            for model in answers:
                all_models.add(model)
                total_answer_count += len(answers[model])
        
        problematic_answers_data = {
            "questions": problematic_answers,
            "summary": {
                "total_questions": len(problematic_answers),
                "total_models": len(all_models),
                "total_answers": total_answer_count,
                "models": sorted(list(all_models)),
                "quality_check_time": datetime.now().isoformat(),
                "quality_issues_summary": {
                    "total_issues": sum(len(issues) for issues in problem_details.values()),
                    "issues_by_question": {q: len(issues) for q, issues in problem_details.items() if q in problematic_answers}
                }
            }
        }
        save_json_file(problematic_answers_data, PROBLEMATIC_ANSWERS_FILE, "æœ‰è´¨é‡é—®é¢˜çš„ç­”æ¡ˆ")
    
    # é‡æ–°è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    def recalculate_stats(grades_list, original_stats):
        if not grades_list:
            return {}
        
        all_scores = [g["avg_scores"]["total"] for g in grades_list]
        all_scores_100 = [g["avg_score_100"] for g in grades_list]
        
        new_stats = original_stats.copy()
        new_stats["total_questions"] = len(grades_list)
        new_stats["valid_grades"] = len(grades_list)
        new_stats["total_average"] = round(sum(all_scores) / len(all_scores), 2) if all_scores else 0
        new_stats["total_average_100"] = round(sum(all_scores_100) / len(all_scores_100), 2) if all_scores_100 else 0
        
        # é‡æ–°è®¡ç®—åˆ†æ•°åˆ†å¸ƒ
        new_stats["score_distribution"] = {
            "0-20": len([s for s in all_scores if s < 20]),
            "20-30": len([s for s in all_scores if 20 <= s < 30]),
            "30-40": len([s for s in all_scores if 30 <= s < 40]),
            "40-50": len([s for s in all_scores if 40 <= s <= 50])
        }
        
        new_stats["separation_time"] = datetime.now().isoformat()
        
        return new_stats
    
    # ä¿å­˜æœ‰é—®é¢˜çš„è¯„åˆ†
    if problematic_grades:
        problematic_data = {
            "metadata": {
                "source_grades_file": str(grades_file),
                "source_answers_file": str(ANSWERS_FILE),
                "separation_reason": "ç­”æ¡ˆè´¨é‡é—®é¢˜å¯¼è‡´è¯„åˆ†ä¸å¯é ",
                "total_issues": sum(len(issues) for issues in problem_details.values()),
                "separation_time": datetime.now().isoformat()
            },
            "statistics": recalculate_stats(problematic_grades, statistics),
            "detailed_results": problematic_grades
        }
        save_json_file(problematic_data, PROBLEMATIC_GRADES_FILE, "æœ‰è´¨é‡é—®é¢˜çš„è¯„åˆ†")
    
    # ä¿å­˜è´¨é‡è‰¯å¥½çš„è¯„åˆ†
    if good_grades:
        good_data = {
            "metadata": {
                "source_grades_file": str(grades_file),
                "source_answers_file": str(ANSWERS_FILE),
                "separation_reason": "ç­”æ¡ˆè´¨é‡è‰¯å¥½ï¼Œè¯„åˆ†å¯é ",
                "separation_time": datetime.now().isoformat()
            },
            "statistics": recalculate_stats(good_grades, statistics),
            "detailed_results": good_grades
        }
        save_json_file(good_data, GOOD_GRADES_FILE, "è´¨é‡è‰¯å¥½çš„è¯„åˆ†")
    
    return len(problematic_grades), len(good_grades)

def generate_quality_report(problematic_questions: Set[str],
                          problem_details: Dict[str, List[Dict]],
                          problematic_count: int,
                          good_count: int):
    """ç”Ÿæˆè´¨é‡æ£€æŸ¥æŠ¥å‘Š"""
    # ç»Ÿè®¡é—®é¢˜ç±»å‹
    issue_types = {}
    model_issues = {}
    
    for question, issues in problem_details.items():
        for issue in issues:
            # ç»Ÿè®¡é—®é¢˜ç±»å‹
            issue_type = issue.get("issue", "æœªçŸ¥")
            issue_types[issue_type] = issue_types.get(issue_type, 0) + 1
            
            # ç»Ÿè®¡æ¨¡å‹é—®é¢˜
            model = issue.get("model", "æœªçŸ¥")
            model_issues[model] = model_issues.get(model, 0) + 1
    
    report = {
        "check_time": datetime.now().isoformat(),
        "files_checked": {
            "answers_file": str(ANSWERS_FILE),
            "grades_file": str(GRADES_FILE)
        },
        "summary": {
            "total_questions_with_issues": len(problematic_questions),
            "total_individual_issues": sum(len(issues) for issues in problem_details.values()),
            "grades_affected": problematic_count,
            "grades_reliable": good_count,
            "reliability_rate": f"{(good_count / (problematic_count + good_count) * 100):.2f}%" if (problematic_count + good_count) > 0 else "0%"
        },
        "issue_types": issue_types,
        "issues_by_model": model_issues,
        "sample_issues": list(problem_details.items())[:10]  # å‰10ä¸ªé—®é¢˜çš„ç¤ºä¾‹
    }
    
    save_json_file(report, QUALITY_CHECK_REPORT, "è´¨é‡æ£€æŸ¥æŠ¥å‘Š")
    
    # æ‰“å°æŠ¥å‘Šæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š è´¨é‡æ£€æŸ¥æŠ¥å‘Šæ‘˜è¦")
    print("="*60)
    print(f"\nğŸ” é—®é¢˜ç±»å‹åˆ†å¸ƒ:")
    for issue_type, count in sorted(issue_types.items(), key=lambda x: x[1], reverse=True):
        print(f"  Â· {issue_type}: {count}")
    
    print(f"\nğŸ¤– å„æ¨¡å‹é—®é¢˜æ•°:")
    for model, count in sorted(model_issues.items(), key=lambda x: x[1], reverse=True):
        print(f"  Â· {model}: {count}")
    
    print(f"\nğŸ“ˆ è¯„åˆ†å¯é æ€§:")
    print(f"  Â· å¯é è¯„åˆ†: {good_count} ({report['summary']['reliability_rate']})")
    print(f"  Â· ä¸å¯é è¯„åˆ†: {problematic_count}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨è¯„åˆ†è´¨é‡åˆ†ç¦»å™¨...")
    print(f"ğŸ“ ç­”æ¡ˆæ–‡ä»¶: {ANSWERS_FILE}")
    print(f"ğŸ“ è¯„åˆ†æ–‡ä»¶: {GRADES_FILE}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    # æ­¥éª¤1: æ£€æŸ¥ç­”æ¡ˆæ–‡ä»¶ï¼Œæ‰¾å‡ºæœ‰é—®é¢˜çš„é¢˜ç›®
    problematic_questions, problem_details, questions_data = check_answers_file(ANSWERS_FILE)
    
    if not problematic_questions:
        print("\nâœ… æ‰€æœ‰é¢˜ç›®çš„ç­”æ¡ˆè´¨é‡éƒ½è‰¯å¥½ï¼Œæ— éœ€åˆ†ç¦»è¯„åˆ†æ–‡ä»¶")
        return
    
    # æ­¥éª¤2: æ ¹æ®è´¨é‡é—®é¢˜åˆ†ç¦»è¯„åˆ†æ–‡ä»¶ï¼Œå¹¶ä¿å­˜æœ‰é—®é¢˜çš„ç­”æ¡ˆ
    problematic_count, good_count = separate_grades_by_quality(
        GRADES_FILE, 
        problematic_questions, 
        problem_details,
        questions_data  # ä¼ é€’åŸå§‹questionsæ•°æ®
    )
    
    # æ­¥éª¤3: ç”Ÿæˆè´¨é‡æ£€æŸ¥æŠ¥å‘Š
    generate_quality_report(
        problematic_questions,
        problem_details,
        problematic_count,
        good_count
    )
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼")
    print(f"  Â· æœ‰é—®é¢˜çš„è¯„åˆ†: {PROBLEMATIC_GRADES_FILE}")
    print(f"  Â· å¯é çš„è¯„åˆ†: {GOOD_GRADES_FILE}")
    print(f"  Â· æœ‰é—®é¢˜çš„ç­”æ¡ˆ: {PROBLEMATIC_ANSWERS_FILE}")
    print(f"  Â· è´¨é‡æŠ¥å‘Š: {QUALITY_CHECK_REPORT}")

if __name__ == "__main__":
    main()