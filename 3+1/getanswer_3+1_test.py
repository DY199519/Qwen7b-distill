#!/usr/bin/env python
# coding: utf-8
"""
txt_qa_processor.py
-----------------
è¯»å–JSONæ ¼å¼çš„é—®é¢˜æ–‡ä»¶ï¼Œåœ¨TXTæ ¼å¼çš„promptæ–‡ä»¶ä¸­åŒ¹é…å¯¹åº”çš„promptï¼Œæ‰§è¡Œæ¨¡å‹è°ƒç”¨ï¼Œç”ŸæˆTXTæ ¼å¼çš„ç­”æ¡ˆæ–‡ä»¶
æ¯ä¸ªpromptç”¨-------------------ä¸¥æ ¼åˆ†å¼€
è¾“å‡ºæ ¼å¼ä¸ºï¼šé—®é¢˜ï¼šXXXX å›å¤:XXXXX
"""

import time
import json
from pathlib import Path
from openai import OpenAI
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import re

# ========== 0. è·¯å¾„é…ç½® =======================================================
# æ§åˆ¶ç”Ÿæˆé¢˜ç›®æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ç”Ÿæˆï¼Œæ•°å­—è¡¨ç¤ºåªç”Ÿæˆå‰Né“é¢˜ï¼‰
LIMIT_QUESTIONS = 2  # ä¾‹å¦‚ï¼š10 è¡¨ç¤ºåªç”Ÿæˆå‰10é“é¢˜ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨ç”Ÿæˆ

# è¾“å…¥è¾“å‡ºæ–‡ä»¶è·¯å¾„
JSON_FILE = Path(r"D:\qwensft\testquestion\multi_model_answersTest500.json")  # JSONé—®é¢˜æ–‡ä»¶è·¯å¾„
TXT_FILE = Path(r"D:\qwensft\uploadjson\final_prompt_3+1-Test.txt")  # TXT promptæ–‡ä»¶è·¯å¾„
OUTPUT_FILE = Path(r"D:\qwensft\uploadjson\final_answer_3+1-Test.txt")  # ä¿®æ”¹ä¸ºä½ çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)

# è¿è¡Œæ—¥å¿—æ–‡ä»¶
RUN_LOG = OUTPUT_FILE.parent / f"run_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"

# ========== 1. æ¨¡å‹é…ç½® =======================================================
MODEL_CFG = {
    "model_name": "deepseek-v3",
    "api_key": "sk-TlCq2TfX7oLuXzZMD1A3681285A2460bA26b6f0cEa5517Aa",
    "base_url": "https://usa.vimsai.com/v1",
    "timeout": 60,  # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "max_retry": 3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
}

# ========== 2. ç­”æ¡ˆéªŒè¯ç±» ====================================================
class AnswerValidator:
    """ç­”æ¡ˆéªŒè¯å™¨"""
    
    # æœ€å°ç­”æ¡ˆé•¿åº¦
    MIN_ANSWER_LENGTH = 5
    
    # é”™è¯¯æ¨¡å¼
    ERROR_PATTERNS = [
        r'^error:',
        r'^exception:',
        r'^\s*$',
        r'^null$',
        r'^undefined$',
        r'^N/A$',
        r'request failed',
        r'rate limit',
        r'timeout',
    ]
    
    @classmethod
    def validate_answer(cls, answer: str) -> Tuple[bool, List[str]]:
        """éªŒè¯ç­”æ¡ˆæ˜¯å¦æœ‰æ•ˆ"""
        issues = []
        
        if not answer:
            issues.append("ç­”æ¡ˆä¸ºç©º")
            return False, issues
        
        if not isinstance(answer, str):
            issues.append(f"ç­”æ¡ˆç±»å‹é”™è¯¯: {type(answer)}")
            return False, issues
        
        answer = answer.strip()
        
        # é•¿åº¦æ£€æŸ¥
        if len(answer) < cls.MIN_ANSWER_LENGTH:
            issues.append(f"ç­”æ¡ˆè¿‡çŸ­ ({len(answer)} å­—ç¬¦)")
        
        # é”™è¯¯æ¨¡å¼æ£€æŸ¥
        for pattern in cls.ERROR_PATTERNS:
            if re.search(pattern, answer, re.IGNORECASE):
                issues.append(f"åŒ¹é…é”™è¯¯æ¨¡å¼: {pattern}")
                return False, issues
        
        return len(issues) == 0, issues

# ========== 3. æ—¥å¿—è®°å½•å™¨ ====================================================
class Logger:
    """ç®€å•çš„æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_file: Path):
        self.log_file = log_file
        self.start_time = datetime.now()
        self._write(f"=== è¿è¡Œå¼€å§‹: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')} ===\n")
    
    def _write(self, message: str):
        """å†™å…¥æ—¥å¿—"""
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(f"{message}\n")
    
    def info(self, message: str):
        """è®°å½•ä¿¡æ¯"""
        timestamp = datetime.now().strftime('%H:%M:%S')
        self._write(f"[{timestamp}] INFO: {message}")
        print(f"ğŸ“ {message}")
    
    def error(self, message: str):
        """è®°å½•é”™è¯¯"""
        timestamp = datetime.now().strftime('%H:%M:%S')
        self._write(f"[{timestamp}] ERROR: {message}")
        print(f"âŒ {message}")
    
    def warning(self, message: str):
        """è®°å½•è­¦å‘Š"""
        timestamp = datetime.now().strftime('%H:%M:%S')
        self._write(f"[{timestamp}] WARNING: {message}")
        print(f"âš ï¸ {message}")
    
    def summary(self, stats: dict):
        """è®°å½•ç»Ÿè®¡æ‘˜è¦"""
        elapsed = datetime.now() - self.start_time
        self._write(f"\n=== è¿è¡Œç»Ÿè®¡ ===")
        self._write(f"æ€»è€—æ—¶: {elapsed}")
        for key, value in stats.items():
            self._write(f"{key}: {value}")
        self._write("=" * 50)

# ========== 4. å·¥å…·å‡½æ•° =======================================================
def read_json_questions(file_path: Path, logger: Logger) -> List[str]:
    """
    è¯»å–JSONæ–‡ä»¶ä¸­çš„é—®é¢˜åˆ—è¡¨
    ä»questionså­—æ®µä¸­æŒ‰é¡ºåºæå–é—®é¢˜
    """
    if not file_path.exists():
        logger.error(f"JSONæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ä»JSONä¸­æå–é—®é¢˜
        questions = []
        if 'questions' in data:
            for question_key in data['questions'].keys():
                questions.append(question_key)
        
        logger.info(f"æˆåŠŸè¯»å– {len(questions)} ä¸ªé—®é¢˜")
        return questions
        
    except Exception as e:
        logger.error(f"è¯»å–JSONæ–‡ä»¶å¤±è´¥: {e}")
        return []

def read_txt_prompts(file_path: Path, logger: Logger) -> Dict[str, str]:
    """
    è¯»å–TXTæ–‡ä»¶ä¸­çš„promptsï¼Œå¹¶å»ºç«‹é—®é¢˜åˆ°promptçš„æ˜ å°„
    æ¯ä¸ªpromptç”¨-------------------åˆ†å¼€
    """
    if not file_path.exists():
        logger.error(f"TXTæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return {}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ä½¿ç”¨-------------------åˆ†å‰²
        prompts = content.split('-------------------')
        
        # å»ºç«‹é—®é¢˜åˆ°promptçš„æ˜ å°„
        question_to_prompt = {}
        for prompt in prompts:
            prompt = prompt.strip()
            if prompt:
                # ä»promptä¸­æå–é—®é¢˜
                question = extract_question_from_prompt(prompt)
                if question:
                    question_to_prompt[question] = prompt
        
        logger.info(f"æˆåŠŸè¯»å– {len(question_to_prompt)} ä¸ªprompts")
        return question_to_prompt
        
    except Exception as e:
        logger.error(f"è¯»å–TXTæ–‡ä»¶å¤±è´¥: {e}")
        return {}

def extract_question_from_prompt(prompt: str) -> str:
    """
    ä»promptä¸­æå–é—®é¢˜éƒ¨åˆ†
    å‡è®¾é—®é¢˜åœ¨promptçš„ç¬¬ä¸€è¡Œæˆ–è€…åŒ…å«"é—®é¢˜"å…³é”®å­—çš„è¡Œ
    """
    lines = prompt.split('\n')
    
    # æŸ¥æ‰¾åŒ…å«"é—®é¢˜"çš„è¡Œ
    for line in lines:
        if 'é—®é¢˜' in line and '"' in line:
            # æå–å¼•å·å†…çš„å†…å®¹
            match = re.search(r'"([^"]+)"', line)
            if match:
                return match.group(1)
    
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›promptçš„å‰100ä¸ªå­—ç¬¦ä½œä¸ºæ ‡è¯†
    return prompt[:100] if len(prompt) > 100 else prompt

def find_matching_prompt(question: str, question_to_prompt: Dict[str, str], logger: Logger) -> Optional[str]:
    """
    åœ¨promptæ˜ å°„ä¸­æŸ¥æ‰¾åŒ¹é…çš„prompt
    """
    # ç›´æ¥åŒ¹é…
    if question in question_to_prompt:
        return question_to_prompt[question]
    
    # æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·ï¼‰
    normalized_question = re.sub(r'[^\w]', '', question)
    for prompt_question, prompt in question_to_prompt.items():
        normalized_prompt_question = re.sub(r'[^\w]', '', prompt_question)
        if normalized_question == normalized_prompt_question:
            logger.info(f"æ¨¡ç³ŠåŒ¹é…æˆåŠŸ: {question[:50]}...")
            return prompt
    
    # éƒ¨åˆ†åŒ¹é…ï¼ˆåŒ…å«å…³ç³»ï¼‰
    for prompt_question, prompt in question_to_prompt.items():
        if question in prompt_question or prompt_question in question:
            logger.info(f"éƒ¨åˆ†åŒ¹é…æˆåŠŸ: {question[:50]}...")
            return prompt
    
    logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„prompt: {question[:50]}...")
    return None

def ask(api: OpenAI, model: str, prompt: str, logger: Logger, 
        timeout: int = 60, max_retry: int = 3, pause: float = 2.0) -> Tuple[str, bool, List[str]]:
    """
    è°ƒç”¨æ¨¡å‹API
    è¿”å›: (ç­”æ¡ˆ, æ˜¯å¦æˆåŠŸ, é”™è¯¯åˆ—è¡¨)
    """
    errors = []
    
    for i in range(1, max_retry + 1):
        try:
            # è®¾ç½®è¶…æ—¶
            rsp = api.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                timeout=timeout
            )
            answer = rsp.choices[0].message.content.strip()
            
            # éªŒè¯ç­”æ¡ˆ
            is_valid, issues = AnswerValidator.validate_answer(answer)
            
            if is_valid and answer:
                return answer, True, []
            else:
                errors.append(f"ç¬¬{i}æ¬¡å°è¯• - ç­”æ¡ˆéªŒè¯å¤±è´¥: {', '.join(issues)}")
                logger.warning(f"ç­”æ¡ˆéªŒè¯å¤±è´¥: {issues}")
                
        except Exception as e:
            error_msg = f"ç¬¬{i}æ¬¡å°è¯•å¤±è´¥: {str(e)}"
            errors.append(error_msg)
            logger.error(error_msg)
        
        if i < max_retry:
            time.sleep(pause * i)  # é€’å¢ç­‰å¾…æ—¶é—´
    
    return "", False, errors

def append_to_output(question: str, answer: str, output_file: Path, logger: Logger):
    """
    å°†é—®ç­”å¯¹è¿½åŠ åˆ°è¾“å‡ºæ–‡ä»¶
    æ ¼å¼ï¼šé—®é¢˜ï¼šXXXX  å›å¤:XXXXX
    """
    try:
        # æ„å»ºè¾“å‡ºå†…å®¹
        output_content = f"é—®é¢˜ï¼š{question}\å‚è€ƒå›å¤ï¼š{answer}\n"
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”æœ‰å†…å®¹ï¼Œæ·»åŠ åˆ†éš”ç¬¦
        if output_file.exists() and output_file.stat().st_size > 0:
            output_content = "-------------------\n" + output_content
        
        # è¿½åŠ åˆ°æ–‡ä»¶
        with open(output_file, 'a', encoding='utf-8') as f:
            f.write(output_content)
        
        logger.info(f"å·²ä¿å­˜ç­”æ¡ˆåˆ°æ–‡ä»¶")
        
    except Exception as e:
        logger.error(f"ä¿å­˜ç­”æ¡ˆå¤±è´¥: {e}")

def check_existing_answers(output_file: Path, logger: Logger) -> set:
    """
    æ£€æŸ¥è¾“å‡ºæ–‡ä»¶ä¸­å·²å­˜åœ¨çš„é—®é¢˜
    è¿”å›å·²å›ç­”é—®é¢˜çš„é›†åˆ
    """
    existing_questions = set()
    
    if not output_file.exists():
        return existing_questions
    
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # åˆ†å‰²å·²æœ‰çš„å›ç­”
        answers = content.split('-------------------')
        
        for answer in answers:
            if 'é—®é¢˜ï¼š' in answer:
                # æå–é—®é¢˜
                match = re.search(r'é—®é¢˜ï¼š(.+?)(?:\n|$)', answer)
                if match:
                    question = match.group(1).strip()
                    existing_questions.add(question)
        
        logger.info(f"å‘ç° {len(existing_questions)} ä¸ªå·²å­˜åœ¨çš„ç­”æ¡ˆ")
        
    except Exception as e:
        logger.error(f"è¯»å–å·²æœ‰ç­”æ¡ˆå¤±è´¥: {e}")
    
    return existing_questions

# ========== 5. ä¸»æ‰§è¡Œå‡½æ•° =====================================================
def run_txt_processing():
    """ä¸»å¤„ç†å‡½æ•°"""
    logger = Logger(RUN_LOG)
    
    logger.info(f"å¼€å§‹å¤„ç†")
    logger.info(f"JSONé—®é¢˜æ–‡ä»¶: {JSON_FILE}")
    logger.info(f"TXT promptæ–‡ä»¶: {TXT_FILE}")
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    logger.info(f"æ¨¡å‹: {MODEL_CFG['model_name']}")
    
    # æ˜¾ç¤ºé¢˜ç›®é™åˆ¶è®¾ç½®
    if LIMIT_QUESTIONS is not None:
        logger.info(f"âš ï¸ è®¾ç½®ç”Ÿæˆé™åˆ¶ï¼šä»…å¤„ç†å‰ {LIMIT_QUESTIONS} é“é¢˜")
    else:
        logger.info(f"å¤„ç†æ‰€æœ‰é¢˜ç›®ï¼ˆæ— é™åˆ¶ï¼‰")
    
    # è¯»å–JSONé—®é¢˜
    questions = read_json_questions(JSON_FILE, logger)
    if not questions:
        logger.error("æ²¡æœ‰è¯»å–åˆ°ä»»ä½•é—®é¢˜")
        return
    
    # è¯»å–TXT prompts
    question_to_prompt = read_txt_prompts(TXT_FILE, logger)
    if not question_to_prompt:
        logger.error("æ²¡æœ‰è¯»å–åˆ°ä»»ä½•prompts")
        return
    
    # åº”ç”¨é¢˜ç›®æ•°é‡é™åˆ¶
    original_count = len(questions)
    if LIMIT_QUESTIONS is not None and LIMIT_QUESTIONS > 0:
        questions = questions[:LIMIT_QUESTIONS]
        logger.info(f"åº”ç”¨é™åˆ¶ï¼šä» {original_count} é“é¢˜ä¸­é€‰æ‹©å‰ {len(questions)} é“é¢˜")
    
    # æ£€æŸ¥å·²å­˜åœ¨çš„ç­”æ¡ˆ
    existing_questions = check_existing_answers(OUTPUT_FILE, logger)
    
    # åˆå§‹åŒ–API
    try:
        api = OpenAI(
            api_key=MODEL_CFG["api_key"], 
            base_url=MODEL_CFG["base_url"]
        )
        logger.info("APIåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"APIåˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "æ¨¡å‹": MODEL_CFG["model_name"],
        "åŸå§‹é¢˜ç›®æ•°": original_count,
        "å¤„ç†é¢˜ç›®æ•°": len(questions),
        "é¢˜ç›®é™åˆ¶": LIMIT_QUESTIONS if LIMIT_QUESTIONS else "æ— é™åˆ¶",
        "è·³è¿‡æ•°": 0,
        "æˆåŠŸæ•°": 0,
        "å¤±è´¥æ•°": 0,
        "æœªåŒ¹é…æ•°": 0,
        "APIè°ƒç”¨æ¬¡æ•°": 0
    }
    
    failed_prompts = []
    unmatched_questions = []
    
    # å¤„ç†æ¯ä¸ªé—®é¢˜
    for idx, question in enumerate(questions, 1):
        print(f"\n[{idx}/{len(questions)}] å¤„ç†é—®é¢˜: {question[:60]}...")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if question in existing_questions:
            print(f"  âœ… è·³è¿‡ï¼ˆå·²æœ‰ç­”æ¡ˆï¼‰")
            stats['è·³è¿‡æ•°'] += 1
            continue
        
        # æŸ¥æ‰¾åŒ¹é…çš„prompt
        prompt = find_matching_prompt(question, question_to_prompt, logger)
        if not prompt:
            print(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…çš„prompt")
            stats['æœªåŒ¹é…æ•°'] += 1
            unmatched_questions.append(question)
            continue
        
        # è°ƒç”¨APIè·å–ç­”æ¡ˆ
        stats['APIè°ƒç”¨æ¬¡æ•°'] += 1
        
        max_attempts = 5
        attempt = 0
        success = False
        answer = ""
        
        while attempt < max_attempts and not success:
            attempt += 1
            if attempt > 1:
                logger.info(f"ç¬¬ {attempt} æ¬¡å°è¯•...")
            
            answer, success, errors = ask(
                api, 
                MODEL_CFG["model_name"], 
                prompt, 
                logger,
                timeout=MODEL_CFG.get('timeout', 60),
                max_retry=MODEL_CFG.get('max_retry', 3)
            )
            
            if success:
                break
            else:
                logger.warning(f"ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥: {errors}")
                if attempt < max_attempts:
                    time.sleep(5 * attempt)
        
        if success:
            stats['æˆåŠŸæ•°'] += 1
            # ä¿å­˜åˆ°æ–‡ä»¶
            append_to_output(question, answer, OUTPUT_FILE, logger)
            print(f"  âœ… æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜ç­”æ¡ˆ")
        else:
            stats['å¤±è´¥æ•°'] += 1
            failed_prompts.append({
                'question': question,
                'prompt': prompt[:200],
                'errors': errors,
                'attempts': attempt
            })
            logger.error(f"é—®é¢˜ '{question[:50]}...' åœ¨ {attempt} æ¬¡å°è¯•åå¤±è´¥")
            
            # å³ä½¿å¤±è´¥ä¹Ÿè®°å½•
            append_to_output(question, f"[ç”Ÿæˆå¤±è´¥: {errors[-1] if errors else 'Unknown error'}]", OUTPUT_FILE, logger)
        
        # æ˜¾ç¤ºè¿›åº¦
        processed = idx
        processed_count = processed - stats['è·³è¿‡æ•°'] - stats['æœªåŒ¹é…æ•°']
        success_rate = (stats['æˆåŠŸæ•°'] / processed_count * 100) if processed_count > 0 else 100
        logger.info(f"è¿›åº¦: {processed}/{len(questions)} (æˆåŠŸç‡: {success_rate:.1f}%)")
        
        # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(1)
    
    # ä¿å­˜å¤±è´¥è®°å½•
    if failed_prompts:
        failed_file = OUTPUT_FILE.parent / f"failed_prompts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(failed_file, 'w', encoding='utf-8') as f:
            for item in failed_prompts:
                f.write(f"é—®é¢˜: {item['question']}\n")
                f.write(f"å°è¯•æ¬¡æ•°: {item['attempts']}\n")
                f.write(f"é”™è¯¯: {item['errors']}\n")
                f.write("-------------------\n")
        logger.warning(f"å¤±è´¥è®°å½•å·²ä¿å­˜åˆ°: {failed_file}")
    
    # ä¿å­˜æœªåŒ¹é…é—®é¢˜è®°å½•
    if unmatched_questions:
        unmatched_file = OUTPUT_FILE.parent / f"unmatched_questions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(unmatched_file, 'w', encoding='utf-8') as f:
            for question in unmatched_questions:
                f.write(f"{question}\n")
        logger.warning(f"æœªåŒ¹é…é—®é¢˜å·²ä¿å­˜åˆ°: {unmatched_file}")
    
    # æ›´æ–°ç»Ÿè®¡
    actual_processed = stats['å¤„ç†é¢˜ç›®æ•°'] - stats['è·³è¿‡æ•°'] - stats['æœªåŒ¹é…æ•°']
    stats['æœ€ç»ˆæˆåŠŸç‡'] = f"{(stats['æˆåŠŸæ•°'] / actual_processed * 100):.1f}%" if actual_processed > 0 else "100%"
    
    # è®°å½•æœ€ç»ˆç»Ÿè®¡
    logger.summary(stats)
    
    print(f"\nğŸ‰ å¤„ç†å®Œæˆï¼")
    print(f"  Â· åŸå§‹æ€»æ•°: {stats['åŸå§‹é¢˜ç›®æ•°']}")
    print(f"  Â· å¤„ç†æ•°é‡: {stats['å¤„ç†é¢˜ç›®æ•°']} (é™åˆ¶: {stats['é¢˜ç›®é™åˆ¶']})")
    print(f"  Â· è·³è¿‡: {stats['è·³è¿‡æ•°']}")
    print(f"  Â· æœªåŒ¹é…: {stats['æœªåŒ¹é…æ•°']}")
    print(f"  Â· æˆåŠŸ: {stats['æˆåŠŸæ•°']}")
    print(f"  Â· å¤±è´¥: {stats['å¤±è´¥æ•°']}")
    print(f"  Â· æˆåŠŸç‡: {stats['æœ€ç»ˆæˆåŠŸç‡']}")

# ========== 6. æ‰§è¡Œå…¥å£ =======================================================
if __name__ == "__main__":
    print("=" * 60)
    print("TXTæ ¼å¼é—®ç­”å¤„ç†è„šæœ¬ (JSONé—®é¢˜ + TXT PromptåŒ¹é…)")
    print("=" * 60)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not JSON_FILE.exists():
        print(f"âŒ JSONé—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {JSON_FILE}")
    elif not TXT_FILE.exists():
        print(f"âŒ TXT promptæ–‡ä»¶ä¸å­˜åœ¨: {TXT_FILE}")
    else:
        run_txt_processing()
        print(f"\nğŸ“ ç»“æœä¿å­˜åœ¨: {OUTPUT_FILE}")
        print(f"ğŸ“ è¿è¡Œæ—¥å¿—: {RUN_LOG}")