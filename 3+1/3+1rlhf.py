#!/usr/bin/env python
# coding: utf-8
"""
direct_rlhf_generator.py
ç›´æ¥ä»åŸå§‹æ•°æ®å’Œè¯„åˆ†æ•°æ®ç”ŸæˆRLHFæ ¼å¼æ•°æ®é›†
ä½¿ç”¨combination_1_promptå’Œcombination_1_replyå­—æ®µ
"""
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Any
import sys
import pandas as pd

# ==================== é…ç½®éƒ¨åˆ† - ä¿®æ”¹è¿™é‡Œ ====================
BASE_DIR = Path(r"D:\project7\10000final")
BASE_DIR1 = Path(r"D:\qwensft\uploadjson")
FUSION_JSON = BASE_DIR / "deepseek_answers_without_summary3+1-1-9400.json"
SCORES_JSON = BASE_DIR / "grades-3+1-1-9400.json"
OUTPUT_BASE = BASE_DIR / "rlhf_train_top100-3+1" # åŸºç¡€æ–‡ä»¶åï¼Œä¼šç”Ÿæˆ .json å’Œ .parquet
TOP_N = 100 # å–è¯„åˆ†æœ€é«˜çš„å‰Nä¸ª
# ===========================================================

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

def load_scores_data(scores_path: Path) -> Dict[str, Dict]:
    """åŠ è½½è¯„åˆ†æ•°æ®"""
    try:
        with scores_path.open("r", encoding="utf-8") as f:
            scores_data = json.load(f)
        
        question_scores = {}
        detailed_results = scores_data.get("detailed_results", [])
        
        for item in detailed_results:
            question = item.get("question", "")
            if question:
                avg_scores = item.get("avg_scores", {})
                avg_score_50 = avg_scores.get("total", 0)
                
                question_scores[question] = {
                    "avg_score_50": avg_score_50,
                    "avg_scores": avg_scores,
                    "num_valid_trials": item.get("num_valid_trials", 0),
                    "avg_logic": avg_scores.get("logic", 0),
                    "avg_depth": avg_scores.get("depth", 0),
                    "avg_innovation": avg_scores.get("innovation", 0),
                    "avg_accuracy": avg_scores.get("accuracy", 0),
                    "avg_completeness": avg_scores.get("completeness", 0),
                }
        
        logger.info(f"æˆåŠŸåŠ è½½äº† {len(question_scores)} ä¸ªé—®é¢˜çš„è¯„åˆ†æ•°æ®")
        
        # æ‰“å°è¯„åˆ†ç»Ÿè®¡
        if question_scores:
            scores_list = [v["avg_score_50"] for v in question_scores.values()]
            non_zero_scores = [s for s in scores_list if s > 0]
            if non_zero_scores:
                logger.info(f"è¯„åˆ†ç»Ÿè®¡: æœ€é«˜ {max(non_zero_scores):.2f}, æœ€ä½ {min(non_zero_scores):.2f}, å¹³å‡ {sum(non_zero_scores)/len(non_zero_scores):.2f}")
        
        return question_scores
        
    except Exception as e:
        logger.error(f"åŠ è½½è¯„åˆ†æ•°æ®å¤±è´¥: {e}")
        return {}

def create_rlhf_sample(item: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°†æ•°æ®é¡¹è½¬æ¢ä¸ºRLHFæ ¼å¼
    ä½¿ç”¨combination_1_promptå’Œcombination_1_replyæ‹¼æ¥ä½œä¸ºsolution
    """
    # problemå°±æ˜¯åŸå§‹çš„question
    problem_text = item.get("question", "")
    
    # è·å–combination_1_promptå’Œcombination_1_reply
    combination_prompt = item.get("combination_1_prompt", "")
    combination_reply = item.get("combination_1_reply", "")
    
    # æ‹¼æ¥solutionï¼Œæ˜ç¡®æ ‡è®°ä¸¤éƒ¨åˆ†ï¼ˆä¸åŸç‰ˆä¿æŒä¸€è‡´çš„æ ¼å¼ï¼‰
    if combination_prompt and combination_reply:
        solution_text = (
            "ã€Combination Promptã€‘\n"
            f"{combination_prompt}\n\n"
            "ã€Combination Replyã€‘\n"
            f"{combination_reply}"
        )
    elif combination_reply:
        solution_text = f"ã€Combination Replyã€‘\n{combination_reply}"
    else:
        solution_text = ""
    
    # æ„å»ºmessages - åŒ…å«å®Œæ•´å¯¹è¯
    messages = [
        {
            "role": "user",
            "content": problem_text
        },
        {
            "role": "assistant",
            "content": solution_text
        }
    ]
    
    # æ„å»ºRLHFæ ¼å¼çš„æ ·æœ¬
    rlhf_sample = {
        "problem": {"Value": problem_text},
        "solution": {"Value": solution_text},
        "messages": {"Value": messages}
    }
    
    return rlhf_sample

def generate_dataset():
    """ç”ŸæˆRLHFæ ¼å¼çš„æ•°æ®é›†"""
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not FUSION_JSON.exists():
        logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {FUSION_JSON}")
        return 1
    
    if not SCORES_JSON.exists():
        logger.error(f"è¯„åˆ†æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {SCORES_JSON}")
        return 1
    
    # åŠ è½½è¯„åˆ†æ•°æ®
    logger.info("æ­£åœ¨åŠ è½½è¯„åˆ†æ•°æ®...")
    question_scores = load_scores_data(SCORES_JSON)
    if not question_scores:
        logger.error("æœªèƒ½åŠ è½½è¯„åˆ†æ•°æ®")
        return 1
    
    # åŠ è½½æ•°æ®
    logger.info("æ­£åœ¨åŠ è½½æ•°æ®...")
    with FUSION_JSON.open("r", encoding="utf-8") as f:
        data = json.load(f)
    
    if isinstance(data, dict):
        items = [data]
    elif isinstance(data, list):
        items = data
    else:
        logger.error(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {type(data)}")
        return 1
    
    logger.info(f"åŠ è½½äº† {len(items)} æ¡æ•°æ®")
    
    # åŒ¹é…è¯„åˆ†å¹¶è¿‡æ»¤æ•°æ®
    items_with_scores = []
    required_fields = ["question", "combination_1_reply"]  # ä¿®æ”¹å¿…éœ€å­—æ®µ
    
    no_score_count = 0
    missing_fields_count = 0
    too_short_count = 0
    
    for item in items:
        question = item.get("question", "")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¯„åˆ†
        if question not in question_scores:
            no_score_count += 1
            continue
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        missing_fields = [f for f in required_fields if not item.get(f)]
        if missing_fields:
            missing_fields_count += 1
            if missing_fields_count <= 3:  # åªæ‰“å°å‰3ä¸ª
                logger.debug(f"ç¼ºå°‘å­—æ®µ {missing_fields}: {question[:50]}...")
            continue
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦
        question_len = len(item.get("question", ""))
        reply_len = len(item.get("combination_1_reply", ""))
        
        if question_len < 10 or reply_len < 100:
            too_short_count += 1
            continue
        
        # æ·»åŠ è¯„åˆ†ä¿¡æ¯
        score_info = question_scores[question]
        item.update(score_info)
        items_with_scores.append(item)
    
    logger.info(f"\næ•°æ®è¿‡æ»¤ç»Ÿè®¡:")
    logger.info(f"  - æ€»æ•°æ®: {len(items)}")
    logger.info(f"  - æ— è¯„åˆ†: {no_score_count}")
    logger.info(f"  - ç¼ºå°‘å­—æ®µ: {missing_fields_count}")
    logger.info(f"  - å†…å®¹è¿‡çŸ­: {too_short_count}")
    logger.info(f"  - æœ‰æ•ˆæ•°æ®: {len(items_with_scores)}")
    
    if not items_with_scores:
        logger.error("æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼")
        return 1
    
    # æŒ‰è¯„åˆ†æ’åºï¼Œå–TOP N
    items_with_scores.sort(key=lambda x: x["avg_score_50"], reverse=True)
    items_to_process = items_with_scores[:TOP_N]
    
    logger.info(f"\nå°†å¤„ç†å¾—åˆ†æœ€é«˜çš„ {len(items_to_process)} æ¡æ•°æ®")
    logger.info("\nTOP 5 é«˜åˆ†é—®é¢˜ï¼ˆ50åˆ†åˆ¶ï¼‰ï¼š")
    for i, item in enumerate(items_to_process[:5], 1):
        score = item["avg_score_50"]
        question = item["question"][:80]
        logger.info(f"  {i}. å¾—åˆ†: {score:.2f}/50 - {question}...")
    
    # è½¬æ¢ä¸ºRLHFæ ¼å¼
    rlhf_dataset = []
    for idx, item in enumerate(items_to_process, 1):
        try:
            rlhf_sample = create_rlhf_sample(item)
            
            if rlhf_sample["problem"]["Value"] and rlhf_sample["solution"]["Value"]:
                rlhf_dataset.append(rlhf_sample)
                
                if idx == 1:
                    logger.info("\nâœ… ç¬¬ä¸€ä¸ªRLHFæ ·æœ¬ç¤ºä¾‹:")
                    logger.info("-" * 60)
                    logger.info(f"Problem (åŸå§‹é—®é¢˜): {rlhf_sample['problem']['Value'][:200]}...")
                    logger.info("-" * 60)
                    logger.info(f"Solutionç»“æ„é¢„è§ˆ:")
                    solution_preview = rlhf_sample['solution']['Value'][:500]
                    logger.info(solution_preview + "...")
                    logger.info("-" * 60)
                
        except Exception as e:
            logger.error(f"å¤„ç†ç¬¬ {idx} ä¸ªæ ·æœ¬å¤±è´¥: {e}")
            continue
    
    if not rlhf_dataset:
        logger.error("æœªèƒ½ç”Ÿæˆä»»ä½•æœ‰æ•ˆçš„RLHFæ ·æœ¬")
        return 1
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    OUTPUT_BASE.parent.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ä¸ºJSONæ ¼å¼
    json_file = OUTPUT_BASE.with_suffix('.json')
    with json_file.open("w", encoding="utf-8") as f:
        json.dump(rlhf_dataset, f, ensure_ascii=False, indent=2)
    logger.info(f"âœ… JSONæ ¼å¼å·²ä¿å­˜åˆ°: {json_file}")
    
    # ä¿å­˜ä¸ºParquetæ ¼å¼
    parquet_file = OUTPUT_BASE.with_suffix('.parquet')
    df_data = []
    for item in rlhf_dataset:
        df_data.append({
            "problem": item["problem"]["Value"],
            "solution": item["solution"]["Value"],
            "messages": item["messages"]["Value"]
        })
    df = pd.DataFrame(df_data)
    df.to_parquet(parquet_file, index=False)
    logger.info(f"âœ… Parquetæ ¼å¼å·²ä¿å­˜åˆ°: {parquet_file}")
    
    logger.info(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(rlhf_dataset)} ä¸ªRLHFè®­ç»ƒæ ·æœ¬")
    
    # ç»Ÿè®¡ä¿¡æ¯
    logger.info("\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    problem_lengths = [len(item["problem"]["Value"]) for item in rlhf_dataset]
    solution_lengths = [len(item["solution"]["Value"]) for item in rlhf_dataset]
    logger.info(f"  - æ ·æœ¬æ•°é‡: {len(rlhf_dataset)}")
    logger.info(f"  - Problemå¹³å‡é•¿åº¦: {sum(problem_lengths)/len(problem_lengths):.0f} å­—ç¬¦")
    logger.info(f"  - Solutionå¹³å‡é•¿åº¦: {sum(solution_lengths)/len(solution_lengths):.0f} å­—ç¬¦")
    logger.info(f"  - Problemæœ€å°/æœ€å¤§: {min(problem_lengths)}/{max(problem_lengths)} å­—ç¬¦")
    logger.info(f"  - Solutionæœ€å°/æœ€å¤§: {min(solution_lengths)}/{max(solution_lengths)} å­—ç¬¦")
    
    # è¾“å‡º Excel: åŒ…å«è¯¦ç»†è¯„åˆ†ä¿¡æ¯
    excel_data = []
    for idx, item in enumerate(items_to_process, 1):
        excel_data.append({
            "åºå·": idx,
            "é—®é¢˜": item["question"],
            "æ€»åˆ†(50åˆ†åˆ¶)": item["avg_score_50"],
            "é€»è¾‘": item.get("avg_logic", 0),
            "æ·±åº¦": item.get("avg_depth", 0),
            "åˆ›æ–°": item.get("avg_innovation", 0),
            "å‡†ç¡®": item.get("avg_accuracy", 0),
            "å®Œæ•´": item.get("avg_completeness", 0),
            "Prompté•¿åº¦": len(item.get("combination_1_prompt", "")),
            "Replyé•¿åº¦": len(item.get("combination_1_reply", "")),
            "Solutionæ€»é•¿åº¦": len(item.get("combination_1_prompt", "")) + len(item.get("combination_1_reply", "")) + 50  # åŠ ä¸Šæ ‡ç­¾é•¿åº¦
        })
    
    df_excel = pd.DataFrame(excel_data)
    excel_file = OUTPUT_BASE.with_suffix('.xlsx')
    df_excel.to_excel(excel_file, index=False)
    logger.info(f"âœ… Excelæ ¼å¼å·²ä¿å­˜åˆ°: {excel_file}")
    
    return 0

def main():
    parser = argparse.ArgumentParser(description="ç”ŸæˆRLHFæ•°æ®é›†")
    parser.add_argument("--top", type=int, help="è¦†ç›–é»˜è®¤çš„TOP_Nå€¼")
    parser.add_argument("--output", type=str, help="è¦†ç›–é»˜è®¤çš„è¾“å‡ºåŸºç¡€æ–‡ä»¶å")
    
    args = parser.parse_args()
    
    # å¦‚æœæä¾›äº†å‘½ä»¤è¡Œå‚æ•°ï¼Œè¦†ç›–é»˜è®¤å€¼
    if args.top:
        global TOP_N
        TOP_N = args.top
    
    if args.output:
        global OUTPUT_BASE
        OUTPUT_BASE = Path(args.output)
    
    logger.info("=" * 70)
    logger.info("RLHFæ•°æ®é›†ç”Ÿæˆå™¨ (Combination_1ç‰ˆæœ¬)")
    logger.info("=" * 70)
    logger.info(f"æ•°æ®æ–‡ä»¶: {FUSION_JSON}")
    logger.info(f"è¯„åˆ†æ•°æ®: {SCORES_JSON}")
    logger.info(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_BASE}.json / .parquet / .xlsx")
    logger.info(f"å¤„ç†æ•°é‡: TOP {TOP_N}")
    logger.info("=" * 70)
    
    return generate_dataset()

if __name__ == "__main__":
    sys.exit(main())